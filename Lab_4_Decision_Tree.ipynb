{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7knjP7oknfjy"
      },
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_RATPcw76pb"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1Q83l1mdJ2xYbGUYicfh8226L2HoO6pBo)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9Mj5SRN_7Xc"
      },
      "source": [
        "## Summary\n",
        "\n",
        "- A Decision Tree is used to build a classification or a regression models in the form of a tree structure\n",
        "- It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed.\n",
        "- The final result is a tree with decision nodes and leaf nodes\n",
        "- The decision nodes are attributes\n",
        "- Branches refer to discrete values (one or more) or intervals for these attributes\n",
        "- Leaves are labeled with classes.\n",
        "- For each leaf, a support and a confidence may be computed:\n",
        "  - Support is the proportion of examples matching the path from root to that leaf\n",
        "  - Confidence is the classification accuracy for examples matching that path.\n",
        "- Each path from the root to a leaf can be used as a class association rule\n",
        "- When passing from decision trees to rules, each rule has the same support and confidence as the leaf from where it comes.\n",
        "- Any example match a single path of the tree (so a single leaf or class)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xApSi11cAHG7"
      },
      "source": [
        "## Pseudocode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuplZKq2XIDQ"
      },
      "source": [
        "```\n",
        "ID3 (Examples, Target_Attribute, Attributes)\n",
        "    Create a root node for the tree\n",
        "    If all member of Examples are in the same class C\n",
        "        Then Return the single-node tree Root with label = C\n",
        "    Else If Attributes is empty\n",
        "        Then Return the single-node tree with label = most common value of Target_Attribute in Examples\n",
        "    Else\n",
        "        A ← The Attribute that best classifies examples. #here we have to choose between ID3 and CART\n",
        "        Decision Tree attribute for Root = A.\n",
        "        For each possible value v of A,\n",
        "            Add a new tree branch below Root, corresponding to the test A = v.\n",
        "            Let Examples(v) be the subset of examples that have the value v for A\n",
        "            If Examples(v) is empty\n",
        "                Then below this new branch add a leaf node with label = most common target value in the examples\n",
        "            Else below this new branch add the subtree ID3 (Examples(v), Target_Attribute, Attributes – {A})\n",
        "    EndIf\n",
        "    Return Root\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJeVSA6jBE2j"
      },
      "source": [
        "## Metrics for Splitting Attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTe0Db0l_Gcp"
      },
      "source": [
        "Input:\n",
        "\n",
        "*   set of k-classes: $C=\\{c_1, c_2, \\dots, c_k\\}$\n",
        "*   labeled dataset: $D=\\{(x_1,c_{1i}), (x_2,c_{2i}), \\dots, (x_n,c_{ni}) \\}$\n",
        "*   each observation contains m features/attributes: $\\{A_1, A_2, \\dots, A_m\\}$\n",
        "\n",
        "\n",
        "**ID3**\n",
        "\n",
        "Entropy of a dataset:\n",
        "*   $Entropy(D) = -\\sum\\limits_{i=1}^{k} p(c_i)*log_{2} p(c_i)$\n",
        "\n",
        "Entropy of a split:\n",
        "- suppose we split by attribute $A_t$ which has **r** distinct values, then it will partition D **r** disjoint subsets: $D = \\cup_{i=1}^r D_i$\n",
        "- $Entropy(D|A_t) = \\sum\\limits_{i=1}^{r}\\frac{|D_i|}{|D|}*Entropy(D_i)$\n",
        "\n",
        "Information Gain:\n",
        "- $InformationGain(D,A_t) = Entropy(D) - Entropy(D|A_t)$\n",
        "\n",
        "Obs:\n",
        "- Minimum value of Entropy will be **0** when all observations belong to one class.\n",
        "- Maximum value of Entropy will be **1** when all target values are equally distributed.\n",
        "\n",
        "**CART**\n",
        "\n",
        "Gini Impurity of a dataset:\n",
        "- $Gini(D) = 1 - \\sum\\limits_{i=1}^{k}\\left(\\frac{p(c_i)}{|D|}\\right)^2$\n",
        "<br><br>\n",
        "\n",
        "Gini Impurity of a split:\n",
        "- $Gini(D|A_t) = \\sum\\limits_{i=1}^{r}\\frac{|D_i|}{|D|}*Gini(D_i)$\n",
        "\n",
        "Gini Gain:\n",
        "- $GiniGain(D,A_t) = Gini(D) - Gini(D|A_t)$\n",
        "\n",
        "Obs:\n",
        "- Minimum value of Gini Impurity will be **0** when all observations belong to one class.\n",
        "- Maximum value of Gini Impurity will be **1** when all target values are equally distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGgugJAkmbiI"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwSGmjdJmuFI"
      },
      "source": [
        "##Download Dataset\n",
        "\n",
        "Download **Play Tennis** dataset. This toy dataset comes with four attributes and the target is to predict based on these attributes if one should play tennis or not.\n",
        "\n",
        "**Attributes:**\n",
        "\n",
        "- outlook $\\in$ [Overcast, Rain, Sunny]\n",
        "- temp $\\in$ [Cool, Hot, Mild]\n",
        "- humidity $\\in$ [Normal, High]\n",
        "- wind $\\in$ [Weak, Strong]\n",
        "\n",
        "**Classes:**\n",
        "- Yes\n",
        "- No\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmkbFho3lvhw",
        "outputId": "3b9e3659-9dec-4bae-e68a-bc9cbefaf47f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "!wget -O play_tennis.csv \"https://drive.google.com/uc?export=download&id=1NT1iJNj3HrPNtiLCb-myrY0XHaJ8_jtf\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-25 10:12:13--  https://drive.google.com/uc?export=download&id=1NT1iJNj3HrPNtiLCb-myrY0XHaJ8_jtf\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.204.102, 172.217.204.138, 172.217.204.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.204.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-6g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vni5uoqnk587vdha6gdko23nc3ihirb6/1571997600000/10258155222664253429/*/1NT1iJNj3HrPNtiLCb-myrY0XHaJ8_jtf?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2019-10-25 10:12:14--  https://doc-00-6g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vni5uoqnk587vdha6gdko23nc3ihirb6/1571997600000/10258155222664253429/*/1NT1iJNj3HrPNtiLCb-myrY0XHaJ8_jtf?e=download\n",
            "Resolving doc-00-6g-docs.googleusercontent.com (doc-00-6g-docs.googleusercontent.com)... 173.194.217.132, 2607:f8b0:400c:c13::84\n",
            "Connecting to doc-00-6g-docs.googleusercontent.com (doc-00-6g-docs.googleusercontent.com)|173.194.217.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 565 [text/csv]\n",
            "Saving to: ‘play_tennis.csv’\n",
            "\n",
            "\rplay_tennis.csv       0%[                    ]       0  --.-KB/s               \rplay_tennis.csv     100%[===================>]     565  --.-KB/s    in 0s      \n",
            "\n",
            "2019-10-25 10:12:14 (24.9 MB/s) - ‘play_tennis.csv’ saved [565/565]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJqW8eneBXmx"
      },
      "source": [
        "## Process Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpW3QOUz0yPf"
      },
      "source": [
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "\n",
        "def train_test_split(df, train_percent=.85):\n",
        "  no_samples, _ = df.shape\n",
        "  no_train_samples = int(0.85 * no_samples)\n",
        "  no_test_samples = no_samples - no_train_samples\n",
        "  train_df = tennis_df.iloc[:no_train_samples, :]\n",
        "  test_df = tennis_df.iloc[-no_test_samples:, :]\n",
        "\n",
        "  X_train = train_df.iloc[:, :-1]\n",
        "  y_train = train_df.iloc[:, -1]\n",
        "  X_test = test_df.iloc[:, :-1]\n",
        "  y_test = test_df.iloc[:, -1]\n",
        "  return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "\n",
        "tennis_df = pd.read_csv(\"play_tennis.csv\", header=0)  \n",
        "\n",
        "#this column is dropped because it bring no meaningful information (it's only a day id)\n",
        "del tennis_df[\"day\"]\n",
        "\n",
        "print(\"Dataset before split\")\n",
        "display(HTML(tennis_df.to_html()))\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tennis_df)\n",
        "\n",
        "print(\"Dataset used for training\")\n",
        "display(HTML(X_train.to_html()))\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "print(\"Dataset used for testing\")\n",
        "display(HTML(X_test.to_html()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCZ7ONrBBlnk"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIBEAZCvoWy1"
      },
      "source": [
        "## E1. ID3 and CART Algorithm\n",
        "\n",
        "ID3 and CART implement the same algorithm presented in the pseudocode. The only difference is the decision where to split. ID3 uses information gain and CART uses gini impurity.\n",
        "ID3 is already implemented.\n",
        "\n",
        "*   **Task 1**: implement predict function\n",
        "The output should be:\n",
        "```\n",
        "['Sunny' 'Hot' 'High' 'Strong'] -> No\n",
        "['Rain' 'Mild' 'High' 'Weak'] -> Yes\n",
        "['Overcast' 'Hot' 'High' 'Strong'] -> Yes\n",
        "```\n",
        "*   **Task 2**: implement gini impurity\n",
        "\n",
        "Find all TODOs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djrocRv1t1gM"
      },
      "source": [
        "#tree structure (already done)\n",
        "class Tree:\n",
        "    def __init__(self, name, data = None):\n",
        "        self.name = name\n",
        "        self.data = data\n",
        "        self.children = []\n",
        "\n",
        "    def setName(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def getName(self):\n",
        "        return self.name\n",
        "    \n",
        "    def setData(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def getData(self):\n",
        "        return self.data\n",
        "\n",
        "    def addChild(self, child, data):\n",
        "        self.children.append({\"split_node\": child, \"split_data\": data})\n",
        "\n",
        "    def printTree(self, level=0):\n",
        "        level += 1\n",
        "        if self.children != []:\n",
        "            for child in self.children:\n",
        "                p = '|'+ ('-'*level*2) + \" \" + self.name + \" \" + str(self.data) + \" == \" + str(child[\"split_data\"])\n",
        "                print(p)              \n",
        "                child[\"split_node\"].printTree(level)\n",
        "        else:\n",
        "            p = '|'+ ('-'*level*2) + \"----> \" + self.name\n",
        "            print(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHzSbvyXqT_4",
        "outputId": "19d5272d-fbcf-4b45-94ec-f50a1e231eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import numpy as np\n",
        "from statistics import mode \n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "  def __init__(self, criterion='entropy'):\n",
        "    self.criterion = criterion\n",
        "  \n",
        "  #converts from Pandas Data Frame -> np.array\n",
        "  def processTrainData(self, X_train_df, y_train_df):\n",
        "    self.attributes = X_train_df.columns.tolist()\n",
        "    self.X_train = X_train_df.values\n",
        "    self.y_train = y_train_df.values\n",
        "    \n",
        "  #converts from Pandas Data Frame -> np.array\n",
        "  def processTestData(self, X_test_df):\n",
        "    self.X_test = X_test_df.values\n",
        "  \n",
        "  #process data and calls a function to build the Decision Tree\n",
        "  def fit(self, X_train_df, y_train_df):\n",
        "    self.processTrainData(X_train_df, y_train_df)\n",
        "    self.dt = self.runAlgorithm(self.X_train, self.y_train, self.attributes)\n",
        "    \n",
        "  def predict(self, ds):\n",
        "    self.processTestData(ds)   \n",
        "    tree = self.dt #do not modify dt\n",
        "    #[TODO] based on tree find the label corresponding to each sample\n",
        "    #print sample -> target value (e.g ['Sunny' 'Hot' 'High' 'Strong'] -> No)\n",
        "    #print(tree.children[0]['split_node'])\n",
        "    for row in [14, 15, 16]:\n",
        "      tree = self.dt\n",
        "      while True:\n",
        "        if(tree.children == []):\n",
        "          line = [ds[col_name][row] for col_name in ds]\n",
        "          print(str(line) + ' ---> ' + tree.getName())\n",
        "          break\n",
        "        for child in tree.children:\n",
        "          #print(ds[tree.getName()][row])\n",
        "          #print(child['split_data'])\n",
        "          if(ds[tree.getName()][row] == child['split_data']):\n",
        "            tree = child['split_node']\n",
        "            break\n",
        "    \n",
        "  def entropy(self, y):\n",
        "    no_classes = len(y)\n",
        "    val, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts/no_classes\n",
        "    h = sum(map(lambda p: -p*np.log2(p), probabilities))\n",
        "    return h\n",
        "  \n",
        "  def gini(self, y):\n",
        "    #[TODO] implement gini\n",
        "    no_classes = len(y)\n",
        "    val, counts = np.unique(y, return_counts=True)\n",
        "    #print(val)\n",
        "    #print(counts)\n",
        "    probabilities = counts / no_classes\n",
        "    return 1 - sum(map(lambda p: p ** 2, probabilities))\n",
        "    #return 1 - sum([p ** 2 for ])\n",
        "  \n",
        "  # func is a function passed as parameter: it can be entropy or gini\n",
        "  def gain(self, x, y, func):\n",
        "    res = func(y)\n",
        "    if res is None:\n",
        "      return None\n",
        "    no_values = len(x)\n",
        "    val, counts = np.unique(x, return_counts=True)\n",
        "    probabilities = counts/no_values\n",
        "    for p, v in zip(probabilities, val):\n",
        "      res  -= p * func(y[x == v])\n",
        "    return res\n",
        "    \n",
        "  def runAlgorithm(self, x, y, attributes):\n",
        "    classes, freqs = np.unique(y, return_counts=True)\n",
        "    \n",
        "    #if all examples begin to the same class => add a leaf node\n",
        "    if len(classes)==1:\n",
        "      RootNode = Tree(classes[0])\n",
        "    #if no more attributes to split => add a leaf node \n",
        "    elif len(attributes)==0:\n",
        "      most_freq_class = mode(y)\n",
        "      RootNode = Tree(most_freq_class)\n",
        "    else:\n",
        "      attr_index = -1\n",
        "      no_attributes = len(attributes)\n",
        "      \n",
        "      #choose function for splitting\n",
        "      if self.criterion == \"entropy\":\n",
        "        func = self.entropy\n",
        "      elif self.criterion == \"gini\":\n",
        "        func = self.gini\n",
        "      else:\n",
        "        raise Exception('Criterion {} not implemented'.format(self.criterion))\n",
        "\n",
        "      #compute attribute A that maximizes information gain\n",
        "      #@attr_index is the index of attribute attr in original list of attributes\n",
        "      #we always want to keep the position of attribute in the original list\n",
        "      #we will use it at predict\n",
        "      #@attr_i iterates through current list of attributes\n",
        "      maxGain = 0\n",
        "      for attr_i, attr in zip(list(range(no_attributes)), attributes):\n",
        "        attr_index_orginal_list = self.attributes.index(attr)\n",
        "        gain = self.gain(x[:, attr_index_orginal_list], y, func)\n",
        "        assert gain is not None, \"Please return something in gain function\"\n",
        "        if(gain > maxGain):\n",
        "          maxGain = gain\n",
        "          attr_index = attr_index_orginal_list\n",
        "        \n",
        "   \n",
        "      #attr becomes decision node\n",
        "      RootNode = Tree(self.attributes[attr_index], attr_index)\n",
        "      \n",
        "      #find all possible values vi of A\n",
        "      unique_values, _ = np.unique(x[:, attr_index], return_counts=True)\n",
        "\n",
        "      #do not edit attributes, make a copy\n",
        "      #this value is shared by all nodes on the same level\n",
        "      new_attributes = attributes[:]\n",
        "      new_attributes.remove(self.attributes[attr_index])\n",
        "\n",
        "      for val in unique_values:\n",
        "        #keep all samples from dataset that have A_t equal with val\n",
        "        new_x = x[x[:, attr_index] == val]\n",
        "        new_y = y[x[:, attr_index] == val]\n",
        "\n",
        "        if(len(new_y) == 0):\n",
        "          #prepare a leaf node with label most common target\n",
        "          most_freq_class = mode(y)\n",
        "          child = Tree(most_freq_class)\n",
        "        else:\n",
        "          #create the subtree from the remaining examples and\n",
        "          #remove the attribute which already created this current branch\n",
        "          child = self.runAlgorithm(new_x, new_y, new_attributes)\n",
        "        RootNode.addChild(child, val)\n",
        "    \n",
        "    return RootNode\n",
        "  \n",
        "  def printDT(self):\n",
        "    self.dt.printTree(0)\n",
        "        \n",
        "\n",
        "tree_id3 = DecisionTreeClassifier(\"gini\")\n",
        "tree_id3.fit(X_train, y_train)\n",
        "tree_id3.printDT()\n",
        "tree_id3.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|-- outlook 0 == Overcast\n",
            "|--------> Yes\n",
            "|-- outlook 0 == Rain\n",
            "|---- wind 3 == Strong\n",
            "|----------> No\n",
            "|---- wind 3 == Weak\n",
            "|----------> Yes\n",
            "|-- outlook 0 == Sunny\n",
            "|---- humidity 2 == High\n",
            "|----------> No\n",
            "|---- humidity 2 == Normal\n",
            "|----------> Yes\n",
            "['Sunny', 'Hot', 'High', 'Strong'] ---> No\n",
            "['Rain', 'Mild', 'High', 'Weak'] ---> Yes\n",
            "['Overcast', 'Hot', 'High', 'Strong'] ---> Yes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}