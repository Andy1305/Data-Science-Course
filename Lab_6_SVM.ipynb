{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTzu5WgHDIEJ"
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from pandas import DataFrame\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "from sklearn.datasets import make_circles\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsVnw8-CLLOm"
      },
      "source": [
        "# Linear Algebra\n",
        "$A = (a_1, a_2, \\dots, a_n)$ \\\\\n",
        "$B = (b_1, b_2, \\dots, b_n)$\n",
        "\n",
        "- **Dot Product** \n",
        "<br>\n",
        "$A \\cdot B = \\sum\\limits_{i=1}^{n}a_i*b_i = a_1*b_1 + a_2*b_2 + \\cdots + a_n*b_n$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZGh31JU7O9E"
      },
      "source": [
        "# Concepts\n",
        "\n",
        "*   ${(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)}$ - dataset D\n",
        "*   $ x_i = (x_{i1}, x_{i2}, \\dots , x_{ir})$ - a single sample with r attributes/features\n",
        "*   $y_i \\in \\{\\color{green}{+1}, \\color{red}{-1}\\}$\n",
        "  *  $\\color{green}{+1}$ - denotes positive class\n",
        "  *  $\\color{red}{-1}$ - denotes negative class\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yBcDI2A-EtG"
      },
      "source": [
        "# SVM\n",
        "\n",
        "- classifier function: \n",
        "$f(x) = \\langle w \\cdot x_i \\rangle + b = w_1*x_1+w_2*x_2*\\cdot*w_n*x_n + b$\n",
        "  - $f:X \\subseteq\t\\Re^r \\rightarrow \\Re$\n",
        "  - $w=(w_1, w_2, \\cdot, w_r) \\in \\Re^r$  - weight vector\n",
        "  - $b \\in \\Re$ - bias vector\n",
        "  - $\\langle w \\cdot x_i \\rangle$ - dot product of w and x (Euclidian inner product)\n",
        "\n",
        "- $x_i$ is assigned to a positive class if $f(x_i)\\ge0$, and to the negative class otherwise \\\\\n",
        "$y_i=\\begin{cases}\n",
        "    \\color{green}{+1},& \\text{if } \\langle w \\cdot x_i \\rangle + b \\geq 0\\\\\n",
        "    \\color{red}{-1},& \\text{if } \\langle w \\cdot x_i \\rangle + b < 0\\\\\n",
        "\\end{cases}$\n",
        "\n",
        "- $\\langle w \\cdot x_i \\rangle + b = 0 \\rightarrow$  becomes the hyperplane(also called decision boundary of decision surface) that separates between positive and negative training examples \n",
        "\n",
        "SVM works only with separable data. In case the data is not linearly separable we make use of kernel functions. In the following sections, both cases are presented.\n",
        "\n",
        "Other problem with the SVM is that is limited to binary classification. This issue can be addressed by using **one-versus-all** classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuVE85Py5-1S"
      },
      "source": [
        "# Linear SVM\n",
        "\n",
        "### Notations\n",
        "- $\\textbf{w}$: vector perpendicular on the plane (normal vector of the hyperplane)\n",
        "- $\\textbf{b}$: used to move hyperplane parallel to itself\n",
        "- $\\color{green}{d_+}|\\color{red}{d_-}$: shortest distance from separating hyperplane closest <font color=\"green\">positive</font> | <font color=\"red\">negative</font> datapoint\n",
        "-  $\\color{green}{X_+}|\\color{red}{X_-}$: support vectors\n",
        "- $\\textbf{margin} = \\color{green}{d_+} + \\color{red}{d_-}$\n",
        "\n",
        "### Objective Function\n",
        "- find the hyperplane with the largest margine\n",
        "- for an indepth mathematical intuition please check the following resources:\n",
        "  - [An Idiot‚Äôs guide to Support vector\n",
        "machines (SVMs)](https://drive.google.com/file/d/1p-l0girUFg8-GuF7IQLKnOLEnOvLpZf4/view?usp=sharing)\n",
        "  - [Lecture 15 - Kernel Methods from Caltech](https://www.youtube.com/watch?v=XUj5JbQihlU)\n",
        "  - [Pattern Recognition and Machine Learning, chapter 7, C. Bishop](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)\n",
        "![](http://drive.google.com/uc?export=view&id=1abobdW1W2HJBXXbnHBYHcXPS_lsvMHj5)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdYEqlPywXwl"
      },
      "source": [
        "The following dataset is generated from scikit-learn, by using [make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) where:\n",
        "- **n_samples**: total number of points generated\n",
        "- **centers**: number of classes (clusters)\n",
        "- **cluster_std**: cluster's standard deviation, if cluster_std is zero, all points from a cluster will overlap\n",
        "- **random_state**: seed used to generate datapoints, if you pass an int it will reproduce the output at every call"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAa8Fsf_Qall"
      },
      "source": [
        "X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.6)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='winter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCxCzdn20gFj"
      },
      "source": [
        "As you can imagine, this datapoints are linearly separable. Linear separability refers to the fact that classses can be separated with a decision surface. (1D data can be separated by a point, 2D can be separated by a line, 3D can be separated by a place, etc.). Now, let's try to draw some lines that can separate this points by finding certain values for the weight and the bias presented above in order to draw decision lines. \n",
        "\n",
        "The idea of this algorithm is to find the best decision lines that separates data for unseen points. Even if each of the tree lines drawn below is separating the actual data we are seeing, this classifier needs to be used also on unseed data. Suppose our new datapoint $\\color{red}{\\times}$ has to be classified by each one of the classifiers(each line is considered a classifier), then the datapoint will be classified as <font color=\"green\">green class</font> if we use the blue classifier, and as <font color=\"blue\">blue class</font> if we use the yellow or violet classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zYzspI-1oXY"
      },
      "source": [
        "no_lines = 3\n",
        "x_values = np.linspace(-1, 4)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='winter')\n",
        "\n",
        "for w, b, c in [(1, 0.65, 'c'), (0.5, 1.6, 'y'), (-0.2, 2.9, 'm')]:\n",
        "  plt.plot(x_values, w*x_values+b, '-k', color=c)\n",
        "plt.plot([0.6], [1.6], 'x', color='red', markeredgewidth=2, markersize=10)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ddvWpV9F2-k"
      },
      "source": [
        "Now, we will train a SVM using a linear kernel for the dataset presented above. All the data plotted as stars represents new data and SVM has not trained on this data. The line in the figure represents the decision boundary that the classifier has learnt during training.\n",
        "\n",
        "[SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) and [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) packages from scikit-learn represent different implementations of the same algorithm. SVC is based on [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) and LinearSVC is based on [liblinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/) and it only supports linear kernel.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyadb0jPGFpF"
      },
      "source": [
        "# create an instance with the classifier\n",
        "svc_model = SVC(kernel='linear')\n",
        "\n",
        "# train classifier\n",
        "svc_model.fit(X, y)\n",
        "\n",
        "# add test samples\n",
        "test_samples = np.array([[0.6, 1.6], [3,5], [1.5, 2.5], [1.5, 2.7]])\n",
        "\n",
        "all_samples = np.concatenate((X, test_samples), axis=0)\n",
        "predicted_results = svc_model.predict(all_samples)\n",
        "\n",
        "figure(num=None, figsize=(8, 6))\n",
        "\n",
        "# print decision boundary and highlight test samples\n",
        "plot_decision_regions(all_samples, predicted_results, \n",
        "                      clf=svc_model, colors=\"blue,green\", \n",
        "                      legend=2, X_highlight=test_samples)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5naBCXsgJSh"
      },
      "source": [
        "Now, we will compute the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) which expects the parameters for true labels and predicted labels. The following values are computes:\n",
        "\n",
        "- True Positives (TP): The class is positive and we have predicted correctly\n",
        "- True Negatives (TN): The class is negative and we have predicted correctly\n",
        "- False Positives (FP): The class is Negative, but we predicted Positive\n",
        "- False Negatives (FN): The class is Positive, but we predicted Negative\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1HGnPRgQYEO49Es6CzQ2pcoamAp7qbXfj)\n",
        "\n",
        "Accuracy is computes as it follows:\n",
        "\n",
        "$Acc = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
        "\n",
        "In our case we will have an accuracy of 100%, because all samples are predicted correctly. Remember, that we are always interested to compute the accuracy also on test dataset which is more important. The classifier has not seen the data from testing, and its guess are more relevant on unseen data. This gives us an intuition if the classifier is able to classify new data. For the moment, we will not compute it firstly the test samples are not labeled and secondly this is a toy dataset to illustrate the intuition behind SVM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B-npD9OYLsE"
      },
      "source": [
        "data = confusion_matrix(y, svc_model.predict(X))\n",
        "plot_confusion_matrix(data)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxZJs-2G6IbG"
      },
      "source": [
        "#Non-Linear SVM\n",
        "\n",
        "When data is not linearly separable, then we have to find a mapping $\\phi$ of the feature space. This mapping transform the N-dimensional feature space into a higher-dimensional space. Basically, it adds a number of dimension which depends on the number of dimensions you already have and are not linearly. The more dimension there are, the more it results in the curse of dimensionality when speaking of mappings. While, training SVM does not need to explicitly apply a mapping function $\\phi$, instead it is happy to have the dot product of $x$ and $x'$ computed in high-dimensional space.\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1DWyZhW0cO_hgdG2ctyifl4gg68jQTZzF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skfmTQkoAeeM"
      },
      "source": [
        "## Kernels\n",
        "\n",
        "Given a non-linear feature space mapping $\\phi(x)$, the kernel function is given by:\n",
        "<br>\n",
        "$k(x,x') = \\phi(x)^T\\phi(x')$\n",
        "<br>\n",
        "If we use a linear kernel function $\\phi(x)=x$, then the we have \n",
        "<br>\n",
        "$k(x,x') = x^Tx'$\n",
        "\n",
        "Kernels:\n",
        "- define a similarity measure (check this [tutorial](https://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/) with Kung-Fu Panda üêº in order to understand what is a similarity measure)\n",
        "- defined by an implicit mapping $\\phi$\n",
        "- is symmetric: $k(x, x')=k(x',x)$\n",
        "\n",
        "Here are the folowing formulas for Kernels according to scikit-learn [implementation](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/svm/src/libsvm/svm.cpp):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8piBgUZC-64"
      },
      "source": [
        "\n",
        "- Linear <br>\n",
        "$k(X, X') = X^T \\cdot X'$\n",
        "- Polynomial <br>\n",
        "$k(X, X') = (\\gamma X^T \\cdot X' + c_0)^p$\n",
        "- RBF <br>\n",
        "$k(X,X') = e^{-\\gamma * ||X-X'||^2}$\n",
        "- Sigmoid <br>\n",
        " $k(X,X') = tanh(\\gamma X  X' + c_0)$\n",
        "\n",
        " For more information, please check the [documentation](https://scikit-learn.org/stable/modules/metrics.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjjAwOZ9W4LT"
      },
      "source": [
        "The next dataset is not linearly separable. There is no line that can separate this datapoints. We will try to apply SVM without using a kernel to see what will SVM classifier will learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYszPw4DD5WZ"
      },
      "source": [
        "X, y = make_circles(n_samples=900, factor=.3, noise=.1, random_state=0)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='winter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvX7_-LilRvb"
      },
      "source": [
        "As we can see, SVM is not capable to find a line that can separate datapoints. That's why we have to call the help of kernel functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE2GIqfbXfJL"
      },
      "source": [
        "# create an instance with the classifier\n",
        "svc_model = SVC(kernel='linear')\n",
        "\n",
        "# train classifier\n",
        "svc_model.fit(X, y)\n",
        "\n",
        "# add test samples\n",
        "predicted_results = svc_model.predict(X)\n",
        "\n",
        "figure(num=None, figsize=(8, 6))\n",
        "\n",
        "# print decision boundary and highlight test samples\n",
        "plot_decision_regions(X, predicted_results, \n",
        "                      clf=svc_model, colors=\"blue,green\", \n",
        "                      legend=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qlWuyrxp6Z2"
      },
      "source": [
        "# create an instance with the classifier\n",
        "svc_model = SVC(kernel='rbf', gamma=0.9)\n",
        "\n",
        "# train classifier\n",
        "svc_model.fit(X, y)\n",
        "\n",
        "# add test samples\n",
        "predicted_results = svc_model.predict(X)\n",
        "\n",
        "figure(num=None, figsize=(8, 6))\n",
        "\n",
        "# print decision boundary and highlight test samples\n",
        "plot_decision_regions(X, predicted_results, \n",
        "                      clf=svc_model, colors=\"blue,green\", \n",
        "                      legend=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAthwmLYs9ID"
      },
      "source": [
        "# Ex 1. XOR Dataset\n",
        "\n",
        "Based on the XOR dataset, train a SVC classifier and check the decision boundaries learnt. Compute accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_H-1ppErg9D"
      },
      "source": [
        "rng = np.random.RandomState(0)\n",
        "X = rng.randn(300, 2)\n",
        "y = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), \n",
        "             dtype=int)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='winter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3URzj-euQTg"
      },
      "source": [
        "# create an instance with the classifier\n",
        "svc_model = SVC(kernel='rbf')\n",
        "\n",
        "# train classifier\n",
        "svc_model.fit(X, y)\n",
        "\n",
        "# add test samples\n",
        "predicted_results = svc_model.predict(X)\n",
        "\n",
        "figure(num=None, figsize=(8, 6))\n",
        "\n",
        "# print decision boundary and highlight test samples\n",
        "plot_decision_regions(X, predicted_results, \n",
        "                      clf=svc_model, colors=\"blue,green\", \n",
        "                      legend=2)\n",
        "plt.show()\n",
        "\n",
        "data = confusion_matrix(y, svc_model.predict(X))\n",
        "FN = data[0][0]\n",
        "FP = data[0][1]\n",
        "TN = data[1][0]\n",
        "TP = data[1][1]\n",
        "\n",
        "acc = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_6fLY3_PxlZ"
      },
      "source": [
        "# Binary Classifier to Multiclass Classifier\n",
        "\n",
        "This sections discusses strategies to use binary classificators in problems where there are more than two classes.\n",
        "\n",
        "Observations:\n",
        "- OVO computes $\\frac{N*(N-1)}{2}$ classifier while OVR computes $N$ classifiers\n",
        "- OVR is trained on inbalanced data, even if classes are balanced, the number of negative samples is larger than the number of positive samples\n",
        "- when using OVR SVMs for multiclass classification, tie cases frequently occur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sntCAVsvY8LT"
      },
      "source": [
        "## OVR (One-vs-Rest)\n",
        "\n",
        "In this strategy we train a classifier for each class, with the samples of the class labeled as positive observations and all other samples labeled as negative observations.\n",
        "\n",
        "- samples: $X=\\{x_1, x_2, \\cdots, x_n\\}$\n",
        "- classes: $y = \\{y_1, y_2, \\cdots, y_k\\}$\n",
        "\n",
        "OVR(model, X, y) \\\\\n",
        "- FOREACH $y_i$ in $y$\n",
        "  - train classifier $f_i$ with $y_i$ as positive class, and all the other classes merged in the negative class\n",
        "  - predict class\n",
        "- choose the classifier which has the positive value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtdnRA2JesAu"
      },
      "source": [
        "## OVO (One-vs-One)\n",
        "\n",
        "In this strategy we train a classifier for each two classes, with the samples from one class labeled as positive observations and all samples from the other class labeled as negative observations.\n",
        "\n",
        "- samples: $X=\\{x_1, x_2, \\cdots, x_n\\}$\n",
        "- classes: $y = \\{y_1, y_2, \\cdots, y_k\\}$\n",
        "\n",
        "OVO(model, X, y) \\\\\n",
        "- FOREACH $y_i$ in $y$, $y_j$ in $y$, $i \\neq j$\n",
        "  - train classifier $f_{i,j}$ with $y_i$ as positive class, and $y_j$ as negative class\n",
        "\n",
        "- a voting schema is applied, where each trained classifier $f_{i,j}$ is applied on the new data and the class which has the most votes will win.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0-ppK2xmBct"
      },
      "source": [
        "# Ex 2. Multiclass Classifier\n",
        "\n",
        "Train a SVM with both OVO and OVR methods based on the below dataset. Compute confusion matrix for each strategy and check decision boundaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-YsqYAkPzJe"
      },
      "source": [
        "X, y = make_blobs(n_samples=300, centers=3, random_state=0, cluster_std=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='winter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZNhgfFWoV6m"
      },
      "source": [
        "# create an instance with the classifier\n",
        "svc_model = SVC(kernel='rbf', decision_function_shape='ovr', gamma=0.9)\n",
        "\n",
        "# train classifier\n",
        "svc_model.fit(X, y)\n",
        "\n",
        "# add test samples\n",
        "predicted_results = svc_model.predict(X)\n",
        "\n",
        "figure(num=None, figsize=(8, 6))\n",
        "\n",
        "# print decision boundary and highlight test samples\n",
        "plot_decision_regions(X, predicted_results, \n",
        "                      clf=svc_model, colors=\"red,blue,green\", \n",
        "                      legend=3)\n",
        "plt.show()\n",
        "data = confusion_matrix(y, svc_model.predict(X))\n",
        "plot_confusion_matrix(data)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# create an instance with the classifier\n",
        "svc_model = SVC(kernel='rbf', decision_function_shape='ovo', gamma=0.9)\n",
        "\n",
        "# train classifier\n",
        "svc_model.fit(X, y)\n",
        "\n",
        "# add test samples\n",
        "predicted_results = svc_model.predict(X)\n",
        "\n",
        "figure(num=None, figsize=(8, 6))\n",
        "\n",
        "# print decision boundary and highlight test samples\n",
        "plot_decision_regions(X, predicted_results, \n",
        "                      clf=svc_model, colors=\"red,blue,green\", \n",
        "                      legend=3)\n",
        "plt.show()\n",
        "data = confusion_matrix(y, svc_model.predict(X))\n",
        "plot_confusion_matrix(data)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5HW2BpHvbGt"
      },
      "source": [
        "# Ex 3. Breast Cancer Dataset\n",
        "\n",
        "Based on [Breast Cancer Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html), predict if a tumor is malign or benign using different markers(attributes) which are mentioned [here](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic). Compare different models:\n",
        "- [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model  \n",
        "- [Decision Tree](https://scikit-learn.org/stable/modules/tree.html) model.\n",
        "\n",
        "Check:\n",
        "- accuracy on train vs test dataset\n",
        "- confusion matrix (because classes might have an unbalanced number of samples)\n",
        "- choose a subset of features/attributes and check if accuracy improves or not\n",
        "\n",
        "Before you start, please analyze the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugNqPC5-vaLo"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#X, y = make_blobs(n_samples=300, centers=3, random_state=0, cluster_std=0.8)\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "#data = pd.DataFrame(np.c_[cancer['data'], cancer['target']], columns=np.append(cancer['feature_names'], ['target']))\n",
        "\n",
        "#X = data.drop(['target'], axis = 1)\n",
        "#y = data['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=20) \n",
        "#plt.scatter(X[:, 0], X[:, 1], c=y, cmap='winter')\n",
        "#plt.show()\n",
        "\n",
        "# create an instance with the classifier\n",
        "svc_model = SVC(kernel='rbf', gamma=0.001)\n",
        "\n",
        "# train classifier\n",
        "svc_model.fit(X_train, y_train)\n",
        "\n",
        "# add test samples\n",
        "predicted_results = svc_model.predict(X_test)\n",
        "data = confusion_matrix(y_test, predicted_results)\n",
        "plot_confusion_matrix(data)\n",
        "\n",
        "cm = np.array(confusion_matrix(y_test, predicted_results))\n",
        "confusion = pd.DataFrame(cm, index=['is_cancer', 'is_healthy'], columns=['predicted_cancer', 'predicted_healthy'])\n",
        "print(confusion)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "clf = DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(y_pred)\n",
        "data = confusion_matrix(y_test, y_pred)\n",
        "plot_confusion_matrix(data)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "#figure(num=None, figsize=(8, 6))\n",
        "\n",
        "# print decision boundary and highlight test samples\n",
        "#plot_decision_regions(X, predicted_results, \n",
        "#                      clf=svc_model, colors=\"blue,green\", \n",
        "#                      legend=2)\n",
        "#plt.show()\n",
        "\n",
        "#data = confusion_matrix(y, svc_model.predict(X))\n",
        "#plot_confusion_matrix(data)\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}