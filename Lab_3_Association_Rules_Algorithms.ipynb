{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4zUQsGr_WaG"
      },
      "source": [
        "# Basic Concepts\n",
        "\n",
        "**Association Rules** represent a class of algorithms in Data Mining. Its objective is to find all co-occurences among data items.\n",
        "\n",
        "\n",
        "**Concepts:**\n",
        "*   $I = \\{i_1, i_2, \\dotso, i_m\\}$ - set of items\n",
        "<br><br>\n",
        "*   $T = \\{t_1, t_2, \\dotso, t_n\\}$ - set of transactions\n",
        "<br><br>\n",
        "*   $X \\rightarrow Y$ where $X \\subset I$, $Y \\subset I$ and $X \\cap Y \\neq \\emptyset$ - association rule; X and Y represent itemset\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**Rule Strength:**\n",
        "\n",
        "*   **Support** - represents the percentage of transaction T that contains $X \\cup Y$ and it is a useful measure in order to state how frequently the itemset appears in the dataset \n",
        "$$supp(X \\rightarrow Y) = Pr(X \\cup Y) = \\frac{count(X \\cup Y)}{|T|} = \\frac{count(X \\cup Y)}{n}$$\n",
        "<br>\n",
        "*   **Confidence** - represents the percentage of rules that contain both X and Y among the rules that contain X and it is a useful measure to determine predictability of a rule\n",
        "$$conf(X \\rightarrow Y) = Pr(Y|X)= \\frac{count(X \\cup Y)}{count(X)}$$\n",
        "\n",
        "* **Lift** - the probability of the itemset Y being purchased when item X \n",
        "\n",
        "$$ lift (X \\rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X)*supp(Y)}$$\n",
        "\n",
        "* **Conviction** - the probability of X occuring in a transaction without Y\n",
        "\n",
        "$$ conv(X \\rightarrow Y) = \\frac{1-supp(Y)}{1-conf(X \\rightarrow Y)}$$\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "Suppose we have the following set of transaction: \n",
        "\n",
        "<font size=\"1\">\n",
        "  \n",
        "| Transaction      | ItemSet         |  \n",
        "| : -------------: |:-------------: |\n",
        "| T1               | A, B , C   | \n",
        "| T2               | A, D, C    |   \n",
        "| T3               | E, A , C      |  \n",
        "| T4               | B , F , E |\n",
        "| T5               | B , D, C, F , E|\n",
        "  \n",
        "</font>\n",
        "\n",
        "\n",
        "Thus, we compute the following :\n",
        "\n",
        "$$supp(A \\rightarrow B ) = \\frac{|\\{T_1\\}|}{5} = \\frac{1}{5} = 0.2$$\n",
        "<br>\n",
        "$$supp (A \\rightarrow C) = \\frac{|\\{T_1, T_2, T_3\\}|}{5} = \\frac{3}{5} = 0.6$$\n",
        "<br>\n",
        "$$conf(A \\rightarrow B) = \\frac{|\\{T_1\\}|}{|\\{T_1,T_2,T_3\\}|} = \\frac{1}{3} = 0.33$$\n",
        "<BR>\n",
        "$$conf(B \\rightarrow C) = \\frac{|\\{T_1,T_5\\}|}{|\\{T_1,T_4,T_5\\}|} = \\frac{22}{3} = 0.66$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiv1CtKDsoBu"
      },
      "source": [
        "# Apriori\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1cIZkclCZ7oirEsL2USqxltnRODk3u86s\"></img>\n",
        "\n",
        ">Apriori is an algorithm used in order to determine frquent itemsets and association rules in transactions. It uses an iterative approach  where $k$-frequent itemsets are used to find $k+1$-frequent itemsets. This algorithm has applications in the well known field Market Basket Analysis.\n",
        "\n",
        "\n",
        "**Steps:**\n",
        "1. **Generate all frequent itemsets** - A frequent itemsets is any itemset from a set of transactions that has $ support \\geq minSupport$\n",
        " - **Downward Closure Property**: If an itemset has minimum support, then\n",
        "every non-empty subset of this itemset also has minimum support.\n",
        " - Candidate List ($C_k$): each candidate list $C_{k}$ is generated by **joining** itemsets from $F_{k-1}$ with itemsets from $F_{k-1}$. The frequent itemsets are joined if they have exactly the same items except the last one. After the **join** step, we have **prunning** step. Here, we eliminate any itemset whose any subset from (k-1) subsets is not in frequent list $F_{k-1}$\n",
        " - Frequent List ($F_k$): this list is generated by eliminating all itemsets from $C_k$ whose support is below **minSupport**\n",
        " - Obs: all itemsets are sorted in lexicographic order, thus we can compare first k-1 elements\n",
        " \n",
        "2. **Generate all confident association rules from the frequent itemsets** - A coffider association rule is any rule that has $confidence \\geq minConf$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya4iLV-kuwnX"
      },
      "source": [
        "##  Pseudocode\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Ao3swsAzEdOrEIkw9cqDCZhh07xNwVa0\" style=\"width=2;\"></img>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86pMRD0dbq2Z"
      },
      "source": [
        "## Example\n",
        "\n",
        "Suppose we have the following table with transactions\n",
        "\n",
        "\n",
        "  \n",
        "| Transaction  | ItemSet |      \n",
        "| :-------------: |:-------------:|\n",
        "| T1 | I1, I2, I5 |\n",
        "| T2 | I2, I4 |\n",
        "| T3 | I2, I3 |\n",
        "| T4 | I1, I2, I4 |\n",
        "| T5 | I1, I3 |\n",
        "| T6 | I2, I3 |\n",
        "| T7 | I1, I3 |\n",
        "| T8 | I1, I2, I3, I5 |\n",
        "| T9 | I1, I2, I3 |\n",
        "  \n",
        "Apriori Steps (minSupport = 2/9):\n",
        "1. <font color='red'><b>\n",
        "  Step 1 (K=1)\n",
        "  </b> </font>\n",
        " \n",
        "- Create candidate list **C1**: we initialize this list with all distinct items\n",
        "   \n",
        "| ItemSet  | Support |\n",
        "| :------: |:-------:|\n",
        "| I1 | 6 |\n",
        "| I2 | 7 |\n",
        "| I3 | 6 |\n",
        "| I4 | 2 |\n",
        "| I5 | 2 |\n",
        "\n",
        "- Create frequent itemsets $F_1$: because each itemset has support at least two, there is no need to eliminate itemsets\n",
        "\n",
        "| ItemSet  | Support |\n",
        "| :------: |:-------:|\n",
        "| I1 | 6 |\n",
        "| I2 | 7 |\n",
        "| I3 | 6 |\n",
        "| I4 | 2 |\n",
        "| I5 | 2 |\n",
        "\n",
        "  \n",
        "2. <font color='red'><b>\n",
        "  Step 2 (K=2)\n",
        "  </b> </font>\n",
        "-  Create candidate list $C_2$: produced by joining itemsets in $F_1$ on lists that have first $k-2=0$ elements in common. There is no subset of any itemsets that has $support < minSupport$. (e.g subsets($[I_1,I_2]$) = $[[I_1],[I_2]]$)\n",
        "\n",
        "| ItemSet  | Support |\n",
        "| :------: |:-------:|\n",
        "| I1, I2 | 4 |\n",
        "| I1, I3 | 4 |\n",
        "| I1, I4 | 1 |\n",
        "| I1, I5 | 2 |\n",
        "| I2, I3 | 4 |\n",
        "| I2, I4 | 2 |\n",
        "| I2, I5 | 2 |\n",
        "| I3, I4 | 0 |\n",
        "| I3, I5 | 1 |\n",
        "| I4, I5 | 0 |\n",
        "\n",
        "- Create frequent itemsets $F_2$: we create $F_2$ by eliminating any itemset that has $support < 2$ in $C_2$\n",
        "\n",
        "| ItemSet  | Support |\n",
        "| :------: |:-------:|\n",
        "| I1, I2 | 4 |\n",
        "| I1, I3 | 4 |\n",
        "| I1, I5 | 2 |\n",
        "| I2, I3 | 4 |\n",
        "| I2, I4 | 2 |\n",
        "| I2, I5 | 2 |\n",
        "\n",
        "  \n",
        "3. <font color='red'><b>\n",
        "  Step 3 (K=3)\n",
        "  </b> </font>\n",
        "- Create candidate list $C_3$: After joining $C_2$ with $C_2$, we observe that certain itemsets have subsets that are not in $F_2$ (e.g subset($[I2,I3,I4]$)=$[[I2,I3], [I2,I4], [I3,I4]]$, here $[I3,I4]$ is not in $F_2$, thus is not a frequent itemset according to **Downward Closure Property** \"a subset of a frequent large set must also be frequent\")\n",
        "\n",
        "| ItemSet  | Support |\n",
        "| :------: |:-------:|\n",
        "| I1, I2, I3 | 2 |\n",
        "| I1, I2, I5 | 2 |\n",
        "\n",
        "- Create frequent itemsets $F_3$\n",
        "\n",
        "| ItemSet  | Support |\n",
        "| :------: |:-------:|\n",
        "| I1, I2, I3 | 2 |\n",
        "| I1, I2, I5 | 2 |\n",
        "\n",
        "4. <font color='red'><b>\n",
        "  Step 4 (K=4)\n",
        "  </b> </font>\n",
        "- Create candidate list $C_4$: The only candidate would be $[I1, I2, I3, I5]$ but it has $support=1$\n",
        "\n",
        "| ItemSet  | Support |\n",
        "| :------: |:-------:|\n",
        "| $\\emptyset$ | $\\emptyset$ |\n",
        "\n",
        "- Create frequent itemsets $F_4$: no $C_4 => $ no $F_4$\n",
        "\n",
        "| ItemSet  | Support |\n",
        "| :------: |:-------:|\n",
        "| $\\emptyset$ | $\\emptyset$ |\n",
        "\n",
        "### Final Result\n",
        " \n",
        " - the list of all frquent itemsets is computed by joining all frquent k-itemsets lists\n",
        " \n",
        " $F=\\cup_k F_k = F_1 \\cup F_2 \\cup F_3 = \\{\n",
        " (I1:6), (I2:7), (I3:6), (I4:2), (I5:2), (I1, I2:4), (I1, I3:4), (I1, I5:2), (I2, I3:4), (I2, I4:2), (I2, I5 :2), (I1, I2, I3:2), (I1, I2, I5:2)\n",
        " \\}$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOlS8vgTcZtQ"
      },
      "source": [
        "# FP Growth\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1tJfzBh8qJWmtI7CH7bsLbmuAK8wT6veh\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBTn-NYfNP6u"
      },
      "source": [
        ">FP growth algorithm is an improvement of apriori algorithm. FP growth algorithm is used for finding frequent itemset in a transaction database without candidate generation.\n",
        "\n",
        ">Advantages of FP growth algorithm:-\n",
        "1. Faster than apriori algorithm\n",
        "2. No candidate generation\n",
        "3. Only two passes over dataset\n",
        "\n",
        ">Disadvantages of FP growth algorithm:-\n",
        "1. FP tree may not fit in memory\n",
        "2. FP tree is expensive to build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv_kUPaIob_7"
      },
      "source": [
        "## Pseudocode\n",
        "\n",
        "### Algorithm 1: [FP-tree construction](https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm)\n",
        "\n",
        ">Input: A transaction database DB and a minimum support threshold ?.\n",
        "\n",
        ">Output: FP-tree, the frequent-pattern tree of DB.\n",
        "\n",
        ">Method: The FP-tree is constructed as follows.\n",
        "\n",
        "    1. Scan the transaction database DB once. Collect F, the set of frequent items, and the support of each frequent item. Sort F in support-descending order as FList, the list of frequent items.\n",
        "    \n",
        "    2. Create the root of an FP-tree, T, and label it as “null”. For each transaction Trans in DB do the following:\n",
        "    - Select the frequent items in Trans and sort them according to the order of FList. Let the sorted frequent-item list in Trans be [ p | P], where p is the first element and P is the remaining list. Call insert tree([ p | P], T ).\n",
        "    - The function insert tree([ p | P], T ) is performed as follows. If T has a child N such that N.item-name = p.item-name, then increment N ’s count by 1; else create a new node N , with its count initialized to 1, its parent link linked to T , and its node-link linked to the nodes with the same item-name via the node-link structure. If P is nonempty, call insert tree(P, N ) recursively.\n",
        "\n",
        "### Algorithm 2: [FP-Growth](https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm)\n",
        "\n",
        "\n",
        ">Input: A database DB, represented by FP-tree constructed according to Algorithm 1, and a minimum support threshold ?.\n",
        "\n",
        ">Output: The complete set of frequent patterns.\n",
        "\n",
        ">Method: call FP-growth(FP-tree, null).\n",
        "\n",
        "    Procedure FP-growth(Tree, a) {      \n",
        "        (01) if Tree contains a single prefix path then { \n",
        "        // Mining single prefix-path FP-tree\n",
        "        (02) let P be the single prefix-path part of Tree;\n",
        "        (03) let Q be the multipath part with the top branching node replaced by a null root;\n",
        "        (04) for each combination (denoted as ß) of the nodes in the path P do\n",
        "        (05) generate pattern ß ∪ a with support = minimum support of nodes in ß;\n",
        "        (06) let freq pattern set(P) be the set of patterns so generated;\n",
        "        }\n",
        "    (07) else let Q be Tree;\n",
        "    (08) for each item ai in Q do { \n",
        "        // Mining multipath FP-tree\n",
        "        (09) generate pattern ß = ai ∪ a with support = ai .support;\n",
        "        (10) construct ß’s conditional pattern-base and then ß’s conditional FP-tree Tree ß;\n",
        "        (11) if Tree ß ≠ Ø then\n",
        "        (12) call FP-growth(Tree ß , ß);\n",
        "        (13) let freq pattern set(Q) be the set of patterns so generated;\n",
        "    }\n",
        "    (14) return(freq pattern set(P) ∪ freq pattern set(Q) ∪ (freq pattern set(P) × freq pattern set(Q)))\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1ZrzEo8i2rw"
      },
      "source": [
        "## Example\n",
        "\n",
        "Suppose we have the following table with transactions and a minSupport = 2/9\n",
        "\n",
        "\n",
        "  \n",
        "| Transaction  | ItemSet |      \n",
        "| :-------------: |:-------------:|\n",
        "| T1 | I1, I2, I5 |\n",
        "| T2 | I2, I4 |\n",
        "| T3 | I2, I3 |\n",
        "| T4 | I1, I2, I4 |\n",
        "| T5 | I1, I3 |\n",
        "| T6 | I2, I3 |\n",
        "| T7 | I1, I3 |\n",
        "| T8 | I1, I2, I3, I5 |\n",
        "| T9 | I1, I2, I3 |\n",
        "  \n",
        "Here are the steps for generating Frequent Patterns:\n",
        "\n",
        "- Create table with support values:\n",
        "\n",
        "| ItemSet  | Support |\n",
        "| :------: |:-------:|\n",
        "| I1 | 6 |\n",
        "| I2 | 7 |\n",
        "| I3 | 6 |\n",
        "| I4 | 2 |\n",
        "| I5 | 2 |\n",
        "\n",
        "- Sort items by support and remove items which do not have $supp \\ge minSupp$. In this case all 1-itemsets have $supp \\ge minSupp$:\n",
        "\n",
        "| ItemSet  | Support |\n",
        "| :------: |:-------:|\n",
        "| I2 | 7 |\n",
        "| I1 | 6 |\n",
        "| I3 | 6 |\n",
        "| I4 | 2 |\n",
        "| I5 | 2 |\n",
        "\n",
        "- Sort transactions and remove items based on the table created at the previous step. Here, we do not need to remove items because all of them have $supp \\ge minSupp$:\n",
        "\n",
        "| Transaction  | ItemSet |      \n",
        "| :-------------: |:-------------:|\n",
        "| T1 | I2, I1, I5 |\n",
        "| T2 | I2, I4 |\n",
        "| T3 | I2, I3 |\n",
        "| T4 | I2, I1, I4 |\n",
        "| T5 | I1, I3 |\n",
        "| T6 | I2, I3 |\n",
        "| T7 | I1, I3 |\n",
        "| T8 | I2, I1, I3, I5 |\n",
        "| T9 | I2, I1, I3 |\n",
        "\n",
        "- Construct FP-Tree\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=19T5dL6vUWIwJu0PxMdFpvnLDEZtx9ATX\"></img>\n",
        "\n",
        "- Generate Frequent Patterns based on FP-Tree\n",
        "\n",
        "| Item | Conditional Pattern Base | Conditional FP-Tree | Frequent Pattern Generated|\n",
        "| :--: | :----------------------: | :-----------------: | :-----------------------: |\n",
        "I1 | {{I2:4}} | \\<I2:4\\> | {I2,I1:4}\n",
        "I2 | $\\emptyset$ | $\\emptyset$ | $\\emptyset$ |\n",
        "I3 | {{I2,I1:2}, {I2:2}, {I1:2}} | \\<I2:4, I1:2\\>, \\<I1:2\\> | {I2, I3: 4}, {I1, I3: 4}, {I2, I1, I3:2}\n",
        "I4 | {{I2,I1:1}, {I2:1}} | \\<I2:2\\> | {I2, I4:2}\n",
        "I5 | {{I2,I1:1}, {I2,I1,I3:1} | \\<I2:2, I1:2\\> | {I2,I5:2}, {I1,I5:2}, {I2,I1, I5:2}, \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzUGCmcwBzBN"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "<font color='red'>Before you start, you should update <b>mlxtend</b> package, since FP-Growth was added in version <b>0.17.0</b> and currently there is a lower version installed. </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex8HQUilGRLy"
      },
      "source": [
        "!pip install --upgrade mlxtend"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hk4SRU0cgon"
      },
      "source": [
        "\n",
        "## Ex1. Apriori\n",
        "Implement **generateFrequentList** and **generateCandidateList** from Apriori algorithm. In order to test your code you can use \n",
        "\n",
        "\n",
        "*   call mlxtend implementation for [Apriori](http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.frequent_patterns/#apriori)\n",
        "*   the following lists outputed for a minSupport=2\n",
        "\n",
        "In the lab implementation you will return a hash for each list that contains itemsets generated in each step (below you have an example).\n",
        "\n",
        "\n",
        "### Candidate List (C)\n",
        "```python\n",
        "[1]: {\n",
        "  'itemsets': [['I1'], ['I2'], ['I3'], ['I4'], ['I5']], \n",
        "  'supp': [6, 7, 6, 2, 2]\n",
        "}\n",
        "[2]: {\n",
        "  'itemsets': [['I1', 'I2'], ['I1', 'I3'], ['I1', 'I4'], ['I1', 'I5'], ['I2', 'I3'], ['I2', 'I4'], ['I2', 'I5'], ['I3', 'I4'], ['I3', 'I5'], ['I4', 'I5']], \n",
        "  'supp': [4, 4, 1, 2, 4, 2, 2, 0, 1, 0]\n",
        "}\n",
        "[3]: {\n",
        "  'itemsets': [['I1', 'I2', 'I3'], ['I1', 'I2', 'I5']], \n",
        "  'supp': [2, 2]\n",
        "}\n",
        "[4]: {\n",
        "  'itemsets': [], \n",
        "  'supp': []}\n",
        "```\n",
        "### Frequent List (F)\n",
        "```python\n",
        "[1]: {\n",
        "  'itemsets': [['I1'], ['I2'], ['I3'], ['I4'], ['I5']], \n",
        "  'supp': [6, 7, 6, 2, 2]\n",
        "}\n",
        "[2]: {\n",
        "  'itemsets': [['I1', 'I2'], ['I1', 'I3'], ['I1', 'I5'], ['I2', 'I3'], ['I2', 'I4'], ['I2', 'I5']], \n",
        "  'supp': [4, 4, 2, 4, 2, 2]\n",
        "}\n",
        "[3]: {\n",
        "  'itemsets': [['I1', 'I2', 'I3'], ['I1', 'I2', 'I5']], \n",
        "  'supp': [2, 2]}\n",
        "[4]: {\n",
        "  'itemsets': [], \n",
        "  'supp': []}\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83oweqUB1yHm"
      },
      "source": [
        "from functools import reduce\n",
        "from itertools import combinations\n",
        "\n",
        "\n",
        "def pretty_print_set(set_param):\n",
        "  for k,v in set_param.items():\n",
        "    print (\"[{0}]: {1}\".format(k,v))\n",
        "\n",
        "transactions = [\n",
        "        [\n",
        "            [\"rule\", \"tree\", \"classification\"],\n",
        "            [\"relation\", \"tuple\", \"join\", \"algebra\", \"recommendation\"],\n",
        "            [\"variable\", \"loop\", \"procedure\", \"rule\"],\n",
        "            [\"clustering\", \"rule\", \"tree\", \"recommendation\"],\n",
        "            [\"join\", \"relation\", \"selection\", \"projection\", \"classification\"],\n",
        "            [\"rule\", \"tree\", \"recommendation\"]\n",
        "        ],\n",
        "        [\n",
        "            [\"I1\", \"I2\", \"I5\"],\n",
        "            [\"I2\", \"I4\"],\n",
        "            [\"I2\", \"I3\"],\n",
        "            [\"I1\", \"I2\", \"I4\"],\n",
        "            [\"I1\", \"I3\"],\n",
        "            [\"I2\", \"I3\"],\n",
        "            [\"I1\", \"I3\"],\n",
        "            [\"I1\", \"I2\", \"I3\", \"I5\"],\n",
        "            [\"I1\", \"I2\", \"I3\"]\n",
        "        ]\n",
        "    ]\n",
        "\n",
        "\n",
        "class Apriori:\n",
        "  def __init__(self, transactions, minSupport):\n",
        "      self.transactions = transactions\n",
        "      self.minSupport = minSupport\n",
        "      self.C = dict()\n",
        "      self.F = dict()\n",
        "  \n",
        "  #converts iterable object into a list of lists\n",
        "  def getListCombinations(self, _list, _k):\n",
        "    return [list(_tuple) for _tuple in list(combinations(_list,_k))]\n",
        "  \n",
        "  #C[k] => F[k]\n",
        "  #generates the list of items who have minimum support\n",
        "  # TODO\n",
        "  def generateFrequentList(self, listC):\n",
        "    listF = {'itemsets':[], 'supp':[]}\n",
        "    assert('itemsets' in listC.keys() and 'supp' in listC.keys())\n",
        "    for (x, y) in zip(listC['itemsets'], listC['supp']):\n",
        "      if(y >= 2):\n",
        "        listF['itemsets'].append(x)\n",
        "        listF['supp'].append(y)\n",
        "    return listF    \n",
        "    \n",
        "  #F[k] => C[k+1]\n",
        "  #generates the candidates list\n",
        "  #  * [join step] C[k+1] = joins(F[k], F[k]) \n",
        "  #                for itemsets who have first (k-1) elements in common\n",
        "  #  * [prunning step] remove from C[k+1] all those itemsets \n",
        "  #                    whose subsets are not frequent\n",
        "  # TODO\n",
        "  def generateCandidateList(self, listF):\n",
        "    listC = {'itemsets':[], 'supp':[]}\n",
        "   # for f1 in listF['itemsets']:\n",
        "   #   for f2 in listF['itemsets']:\n",
        "    items = listF['itemsets']\n",
        "    print(items)\n",
        "    for i in range(len(items)):\n",
        "      for j in range(i + 1, len(items)):\n",
        "        for (e1, e2) in zip(items[i], items[j]):\n",
        "          #print(items[i], items[j])\n",
        "          if e1 != e2 and items[i][len(items[i]) - 1] == e1:\n",
        "            merged_list = list(dict.fromkeys(items[i] + items[j]))\n",
        "            print(listC['itemsets'])\n",
        "            print(merged_list)\n",
        "            listC['itemsets'].append(merged_list)\n",
        "            #listC['supp'].append(self.getSupport(items[i]))\n",
        "            \n",
        "    assert('itemsets' in listF.keys() and 'supp' in listF.keys())\n",
        "    \n",
        "        \n",
        "    return listC\n",
        "  \n",
        "  def getSupport(self, itemsets):\n",
        "    supp = []\n",
        "    for item in itemsets:\n",
        "      supp_item = 0\n",
        "      for t in self.transactions:\n",
        "        if set(item).issubset(set(t)):\n",
        "          supp_item += 1\n",
        "      supp.append(supp_item)\n",
        "    return supp\n",
        "    \n",
        "  def runAlgorithm(self):\n",
        "    set_unique_items = reduce(lambda l1, l2: l1.union(l2), self.transactions, set())\n",
        "    self.C[1] = {}\n",
        "    self.C[1]['itemsets'] = [[elem] for elem in sorted(set_unique_items)]\n",
        "    self.C[1]['supp'] = self.getSupport(self.C[1]['itemsets'])\n",
        "    self.F[1] = self.generateFrequentList(self.C[1])\n",
        "    \n",
        "    k = 1\n",
        "    while(len(self.F[k]['itemsets']) != 0):\n",
        "      self.C[k+1] = {}\n",
        "      self.C[k+1] = self.generateCandidateList(self.F[k])\n",
        "      self.F[k+1] = self.generateFrequentList(self.C[k+1])\n",
        "      k += 1\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tid = 1\n",
        "    minSupport = 2\n",
        "    apriori = Apriori(transactions[tid], minSupport)\n",
        "    apriori.runAlgorithm()\n",
        "    print(\"=== Candidate List ===\")\n",
        "    pretty_print_set(apriori.C)\n",
        "    print(\"=== Frequent List Items ===\")\n",
        "    pretty_print_set(apriori.F)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuh6CDeLGpi7"
      },
      "source": [
        "## Ex2. FP-Growth\n",
        "\n",
        "Implement Apriori algorithm using mlxtend implementation for [FP-Growth](http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.frequent_patterns/#fpgrowth). Generate frequent patterns and apply associations rules.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rahY8JmTFiLG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6NQ1UeQricO"
      },
      "source": [
        "## Ex3. CAR for Titanic Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwK45wEUrzBC"
      },
      "source": [
        "Apply Apriori or FP-Growth on [Titanic Dataset](https://www.kaggle.com/c/titanic/data). Use the following features: Sex, Age, Class, Adult, Survived"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_NSNOUytbm1"
      },
      "source": [
        "!wget -O titanic.csv https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7giVggtTt6pb"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "from mlxtend.frequent_patterns import fpgrowth\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "#expects Pandas DF\n",
        "def go(title, df_result):\n",
        "  df_str = df_result.to_string().split('\\n')\n",
        "  max_len = max(map(len, df_str))\n",
        "  half_len = int((max_len-len(title)-1)/2)\n",
        "  half_len = half_len if half_len else 1\n",
        "  print(\"-\" * half_len, title, \"-\" * half_len)\n",
        "  print(df_result.to_string())\n",
        "  print(\"\\n\")\n",
        "\n",
        "  \n",
        "  \n",
        "large_titanic_df = pd.read_csv(\"titanic.csv\")\n",
        "titanic_df = large_titanic_df[['Sex', 'Age', 'Survived', 'Pclass']].copy()\n",
        "\n",
        "#check if there are null values\n",
        "#if there are we need to approximate value\n",
        "#lucky us, there are not\n",
        "go(\"Null Values\", titanic_df.isnull().sum())\n",
        "\n",
        "\n",
        "#analyze data\n",
        "#categorical columns: Sex, Pclass, Survived\n",
        "#numerical column: Age\n",
        "go(\"Sex\",titanic_df['Sex'].value_counts())\n",
        "go(\"Age\", titanic_df[\"Age\"].describe())\n",
        "go(\"Pclass\", titanic_df[\"Pclass\"].value_counts())\n",
        "go(\"Survived\", titanic_df[\"Survived\"].value_counts())\n",
        "\n",
        "#in order to run Apriori, we need to reduce our values \n",
        "#to a smaller number of categories\n",
        "#if not, think of how many infrequent rules there will be\n",
        "#binning technique (manareala)\n",
        "#age in inteval [0,13] => Age=Baby, etc.\n",
        "\n",
        "titanic_df[\"Age\"] = pd.cut(titanic_df[\"Age\"], [0,18,61,80], \n",
        "                           labels=[ 'Age=Child', 'Age=Adult', 'Age=Senior'])\n",
        "\n",
        "\n",
        "#prettify values in columns to understand something from the generated rules\n",
        "titanic_df['Sex'] = titanic_df['Sex'].replace(['male','female'],\n",
        "                                              ['Sex=Male','Sex=Female'])\n",
        "\n",
        "titanic_df['Survived'] = titanic_df['Survived'].replace([0,1],\n",
        "                                              ['Survived=Nope','Survived=Yeap'])\n",
        "\n",
        "titanic_df['Pclass'] = titanic_df['Pclass'].replace([1,2,3],\n",
        "                                               ['Class=1','Class=2','Class=3'])\n",
        "\n",
        "\n",
        "\n",
        "#TransactionEncoder receives a pythonic list\n",
        "#will rename all items in weird alphabet characters\n",
        "#=> need to convert from Data Frame to list\n",
        "\n",
        "titanic_list = []\n",
        "for sublist in titanic_df.values.tolist():\n",
        "    clean_sublist = [item for item in sublist]\n",
        "    titanic_list.append(clean_sublist)\n",
        "    \n",
        "\n",
        "\n",
        "#transform data for algorithm\n",
        "#list -> into a one-hot encoded NumPy boolean array\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(titanic_list).transform(titanic_list)\n",
        "df_bool = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "\n",
        "\n",
        "#TODO: Run apriori or FP-Growth\n",
        "frequent_itemsets=......\n",
        "\n",
        "\n",
        "#we are interested only in those rules that contain the survival information\n",
        "alive_df = frequent_itemsets[frequent_itemsets['itemsets'].astype(str).str.contains('Survived=Yeap')]\n",
        "dead_df = frequent_itemsets[frequent_itemsets['itemsets'].astype(str).str.contains('Survived=Nope')]\n",
        "all_df = pd.concat([alive_df, dead_df])\n",
        "all_df.sort_values(by='support', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}