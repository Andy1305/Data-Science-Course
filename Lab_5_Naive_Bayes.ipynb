{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z77ChK6uTel"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcQdyf7UziIo"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Law of Total Probability\n",
        "\n",
        "#### Definition\n",
        "`The total probability rule (also called the Law of Total Probability) breaks up probability calculations into distinct parts. It‚Äôs used to find the probability of an event, A, when you don‚Äôt know enough about A‚Äôs probabilities to calculate it directly. Instead, you take a related event, B, and use that to calculate the probability for A.`\n",
        "\n",
        "$$P(A)=\\sum_{i=1}^{n}P(A|B_i)*P(B_i)$$\n",
        "\n",
        "####Example\n",
        "We have two bags:\n",
        "* Bag 1 has 2 red traingles, 2 green squares and 1 blue circle\n",
        "* Bas 2 has 1 red triangle, 1 green square and 2 blue circles\n",
        "\n",
        "If we choose one bag at random and pick something from the bag also randomly, then what is the probability of choosing a red triangle?\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1KxmicbEdMXBHVzP_WzASSK_hgOnnFOSd)\n",
        "\n",
        "####Solution\n",
        "- let $P(B_i)$ be the probability of choosing bag $B_i$\n",
        "- let R be the event of choosing a red triangle\n",
        "\n",
        "$P(B_a) = P(B_b) = \\frac{1}{2} = 0.5$\n",
        "\n",
        "$P(R|B_a) = \\frac{2}{5} = 0.4$\n",
        "\n",
        "$P(R|B_b) = \\frac{1}{4} = 0.25$\n",
        "\n",
        "\n",
        "By applying Law of Total Probability we can compute the final probability:\n",
        "\n",
        "$P(R) = P(R|B_a)*P(B_a) + P(R|B_b)*P(B_b) = 0.4*0.5 + 0.25*0.5 = 0.325$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6X6Y1f60gg"
      },
      "source": [
        "### Dependent vs Independent Event\n",
        "\n",
        "- if events are independent, it means that each event is not affected by any other event(e.g. coin toss - the past does not affect the current toss)\n",
        "- events are dependent if a past event will affect the current event (e.g drawing two Kings from a deck)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY5x6sFdzkiI"
      },
      "source": [
        "### Bayes' Theorem\n",
        "\n",
        "- $\\color{green}{P(A|B)} = \\frac{\\color{blue}{P(B|A)} * \\color{orange}{P(A)}}{\\color{LightSeaGreen}{P(B)}}$ \n",
        "<br><br>\n",
        "- $\\color{green}{posterior} = \\frac{\\color{blue}{prior} \\times \\color{orange}{\\color{orange}{likelihood}}}{\\color{LightSeaGreen}{evidence}}$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgE0-RgX8KFj"
      },
      "source": [
        "### Target\n",
        "\n",
        "The objective is to build a classifier which is capable to predict a class based on the inputs given (given sample $x_i$ predict to which class it belongs).\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7TRLD4Au0vg"
      },
      "source": [
        "### Concepts\n",
        "- $X=\\{x_1, x_2, \\dots, x_n\\}$: dataset with **n** elements\n",
        "- $A=\\{A_1, A_2, \\dots, A_m\\}$: each $x_i$ is described by **m** features/attributes, thus $x_i=\\{a_1^{(i)}, a_2^{(i)}, \\dots, a_m^{(i)}\\}$\n",
        "- $C=\\{c_1, c_2, \\dots, c_k\\}$: set of classes\n",
        "- $P(C = c_j|x = x_i)$: the probability of a sample $x_i$ to belong to class $c_j$\n",
        "- for simplicity $P(A_1=a_1^{(i)}, A_2=a_2^{(i)}, \\dots, A_m=a_m^{(i)}|C=c_j)$ will be written as $P(a_1^{(i)}, a_2^{(i)}, \\dots, a_m^{(i)}|C=c_j)$ \n",
        "<br><br>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SVF6EZx1Q9r"
      },
      "source": [
        "### Naive Bayes Classifier Formalism\n",
        "\n",
        "\n",
        "*   Objective function to compute: $\\color{green}{P(C = c_j | x = x_i)}$\n",
        "*   Apply Bayes Theorem to compute the above probability:\n",
        "$$\\color{green}{P(C=c_j|x=x_i)} = \\frac{\\color{blue}{P(x=x_i|C=c_j)}\\times \\color{orange}{P(C=c_j)}}{\\color{LightSeaGreen }{P(x=x_i)}}$$\n",
        "*   Now, we will focus only on priori probability $ \\color{blue}{P(x=x_i|C=c_j)}$ which will be rewritten below :\n",
        "$\\color{blue}{P(x=x_i|C=c_j)} = \\\\\n",
        "= P(a_1^{(i)}, a_2^{(i)}, \\dots, a_m^{(i)}|C=c_j) \\\\ \n",
        "= P(a_1^{(i)} | a_2^{(i)}, \\dots, a_m^{(i)}, C=c_j) \\times P( a_2^{(i)}, \\dots, a_m^{(i)}, C=c_j) \\\\\n",
        "= P(a_1^{(i)} | a_2^{(i)}, \\dots, a_m^{(i)}, C=c_j) \\times P( a_2^{(i)}| a_3^{(i)}, \\dots, a_m^{(i)}, C=c_j) \\times P(a_3^{(i)}, \\dots, a_m^{(i)}, C=c_j) \\\\\n",
        "= P(\\color{red}{a_1^{(i)}} | a_2^{(i)}, \\dots, a_m^{(i)}, C=c_j) \\times P(\\color{red}{a_2^{(i)}} | a_3^{(i)}, \\dots, a_m^{(i)}, C=c_j) \\times \\dots \\times P(\\color{red}{a_{m-1}^{(i)}} |a_m^{(i)}, C=c_j) \\times  P(\\color{red}{a_m^{(i)}} | C=c_j) \n",
        "$\n",
        "<br><br>\n",
        "- Apply Naive Bayes assumption and rewrite $P(x=x_i|C=c_j)$: \\\\\n",
        "$\\color{blue}{P(x=x_i|C=c_j)} =\\\\\n",
        "= P(\\color{red}{a_1^{(i)}}, C=c_j) \\times P(\\color{red}{a_2^{(i)}}, C=c_j) \\times \\dots \\times P(\\color{red}{a_m^{(i)}}, C=c_j)  \\\\\n",
        "=   \\prod\\limits_{\\alpha=1}^m   P(\\color{red}{a_\\alpha^{(i)}}, C=c_j)  \n",
        "$\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1kSmOVBKaMmkpgs6eXs1wcYdFuHWklWU4)\n",
        "\n",
        "- The main idea of Naive Bayes is that it comes with the assumption that all attributes/features($a_1^{(i)},\\dots,a_m^{(i)}$) are conditionally independent. A more concrete example would be to classify a collection of documents into topics like politics, science, etc. Given a document **D** which is composed of words/tokens and a list of classes C={politics, science} one will choose politics as the main topic in document D. By using Naive Bayes, the assumption made is that words in document D are conditionally independent and that is why the algorithm is called naive. In English grammar as any other language, there are rules like ``adjectives usually comes before noun`` or ``sentences start with a capital letter``, etc. Thus, each word depends on the previous words. \\\\\n",
        "``D=President Trump and first lady Melania Trump handed out commemorative Halloween candy to trick-or-treaters at the White House Monday``\n",
        "\n",
        "- Now, we will focus on denominator(evidence) $\\color{LightSeaGreen}{P(x=x_i)}$ and apply Law of Total Probability: \\\\\n",
        "$\\color{LightSeaGreen}{P(x=x_i)} = \\sum\\limits_{\\beta=1}^{k} \\color{blue}{P(x=x_i|C=c_\\beta)} \\times P(C=c_\\beta)$\n",
        "- But we have computed already the formulas for $\\color{blue}{P(x=x_i|C=c_\\beta)}$, thus $\\color{LightSeaGreen}{P(x=x_i)}$ becomes: \\\\\n",
        "$\\color{LightSeaGreen}{P(x=x_i)} = \\sum\\limits_{\\beta=1}^{k} \\left[ \\prod\\limits_{\\alpha=1}^m   P(\\color{red}{a_\\alpha^{(i)}}, C=c_\\beta) \\times P(C=c_\\beta)  \\right] $\n",
        "\n",
        "- After all this computations we get the following formulas: \\\\\n",
        "<br>\n",
        "$\\color{green}{P(C=c_j|x=x_i)} = \\frac{\\color{blue}{P(x=x_i|C=c_j)}\\times \\color{orange}{P(C=c_j)}}{\\color{LightSeaGreen }{P(x=x_i)}} = \n",
        "\\frac{\\left[ \\prod\\limits_{\\alpha=1}^m   P(\\color{red}{a_\\alpha^{(i)}}, C=c_j) \\right] \\times \\color{orange}{P(C=c_j)}}{ \\sum\\limits_{\\beta=1}^{k} \\left[ \\prod\\limits_{\\alpha=1}^m   P(\\color{red}{a_\\alpha^{(i)}}, C=c_\\beta) \\times P(C=c_\\beta)  \\right]}\n",
        "$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMdVh_YD71wT"
      },
      "source": [
        "### Naive Bayes Classifier\n",
        "\n",
        "<a id='naive_bayes_cell'></a>\n",
        "\n",
        "- When only classification is needed, the denominator of the above expression may be ignored (is the same for all ùëêùëó) and the labeling class is obtained by maximizing the numerator: \\\\\n",
        "$Class(x_i) = argmax_{c_j} \\Bigg\\{ \\left[ \\prod\\limits_{\\alpha=1}^m   P(\\color{red}{a_\\alpha^{(i)}}, C=c_j) \\right] \\times \\color{orange}{P(C=c_j)} \\Bigg\\}$\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_pFdNEcN7xA"
      },
      "source": [
        "## Example\n",
        "\n",
        "<table width=\"100%\" style=\"table-layout:fixed;\" >\n",
        "<thead>\n",
        "<tr>\n",
        "<th style=\"padding:8px;background-color:#4CB96B;text-align:center;\"> </th>\n",
        "<th style=\"padding:8px;background-color:#4CB96B;text-align:center;\">Outlook</th>\n",
        "<th style=\"padding:8px;background-color:#4CB96B;text-align:center;\">Temperature</th>\n",
        "<th style=\"padding:8px;background-color:#4CB96B;text-align:center;\">Humidity</th>\n",
        "<th style=\"padding:8px;background-color:#4CB96B;text-align:center;\">Windy</th>\n",
        "<th style=\"padding:8px;background-color:#4CB96B;text-align:center;\">Play Golf</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td style=\"text-align:center;\">0</td>\n",
        "<td style=\"text-align:center;\">Sunny</td>\n",
        "<td style=\"text-align:center;\">Hot</td>\n",
        "<td style=\"text-align:center;\">High</td>\n",
        "<td style=\"text-align:center;\">Weak</td>\n",
        "<td style=\"text-align:center;\">No</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">1</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Sunny</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Hot</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">High</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Strong</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">No</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center;\">2</td>\n",
        "<td style=\"text-align:center;\">Overcast</td>\n",
        "<td style=\"text-align:center;\">Hot</td>\n",
        "<td style=\"text-align:center;\">High</td>\n",
        "<td style=\"text-align:center;\">Weak</td>\n",
        "<td style=\"text-align:center;\">Yes</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">3</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Rain</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Mild</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">High</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Weak</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Yes</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center;\">4</td>\n",
        "<td style=\"text-align:center;\">Rain</td>\n",
        "<td style=\"text-align:center;\">Cool</td>\n",
        "<td style=\"text-align:center;\">Normal</td>\n",
        "<td style=\"text-align:center;\">Weak</td>\n",
        "<td style=\"text-align:center;\">Yes</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">5</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Rain</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Cool</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Normal</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Strong</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">No</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center;\">6</td>\n",
        "<td style=\"text-align:center;\">Overcast</td>\n",
        "<td style=\"text-align:center;\">Cool</td>\n",
        "<td style=\"text-align:center;\">Normal</td>\n",
        "<td style=\"text-align:center;\">Strong</td>\n",
        "<td style=\"text-align:center;\">Yes</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">7</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Sunny</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Mild</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">High</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Weak</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">No</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center;\">8</td>\n",
        "<td style=\"text-align:center;\">Sunny</td>\n",
        "<td style=\"text-align:center;\">Cool</td>\n",
        "<td style=\"text-align:center;\">Normal</td>\n",
        "<td style=\"text-align:center;\">Weak</td>\n",
        "<td style=\"text-align:center;\">Yes</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">9</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Rain</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Mild</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Normal</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Weak</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Yes</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center;\">10</td>\n",
        "<td style=\"text-align:center;\">Sunny</td>\n",
        "<td style=\"text-align:center;\">Mild</td>\n",
        "<td style=\"text-align:center;\">Normal</td>\n",
        "<td style=\"text-align:center;\">Strong</td>\n",
        "<td style=\"text-align:center;\">Yes</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">11</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Overcast</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Mild</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">High</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Strong</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Yes</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center;\">12</td>\n",
        "<td style=\"text-align:center;\">Overcast</td>\n",
        "<td style=\"text-align:center;\">Hot</td>\n",
        "<td style=\"text-align:center;\">Normal</td>\n",
        "<td style=\"text-align:center;\">Weak</td>\n",
        "<td style=\"text-align:center;\">Yes</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">13</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Rain</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Mild</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">High</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">Strong</td>\n",
        "<td style=\"background-color:rgba(183,223,182,0.4);text-align:center;\">No</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqGVojQ5RF5Z"
      },
      "source": [
        "*   We have two classes: [<font color=\"green\">Yes</font>, <font color=\"red\">No</font>]\n",
        "  *   P(Play=<font color=\"green\">Yes</font>) = 9/14\n",
        "  *   P(Play=<font color=\"red\">No</font>) = 5/14\n",
        "\n",
        "1. Training Info $\\rightarrow$ compute all probabilities \n",
        "<br><br>\n",
        "<table width=\"400\" border=\"1\" cellspacing=\"0\" cellpadding=\"2\">\n",
        "<tbody>\n",
        "<tr>\n",
        "\n",
        "<td valign=\"top\" width=\"133\"><b>OUTLOOK</b></td>\n",
        "<td valign=\"top\" width=\"133\"><b>Play = Yes</b></td>\n",
        "<td valign=\"top\" width=\"133\"><b>Play = No</b></td>\n",
        "<td valign=\"top\" width=\"133\"><b>Total</b></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td valign=\"top\" width=\"133\">Sunny</td>\n",
        "<td valign=\"top\" width=\"133\">2/9</td>\n",
        "<td valign=\"top\" width=\"133\">3/5</td>\n",
        "<td valign=\"top\" width=\"133\">5/14</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td valign=\"top\" width=\"133\">Overcast</td>\n",
        "<td valign=\"top\" width=\"133\">4/9</td>\n",
        "<td valign=\"top\" width=\"133\">0/5</td>\n",
        "<td valign=\"top\" width=\"133\">4/14</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td valign=\"top\" width=\"133\">Rain</td>\n",
        "<td valign=\"top\" width=\"133\">3/9</td>\n",
        "<td valign=\"top\" width=\"133\">2/5</td>\n",
        "<td valign=\"top\" width=\"133\">5/14</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "<br><br>\n",
        "<table width=\"400\" border=\"1\" cellspacing=\"0\" cellpadding=\"2\">\n",
        "<tbody>\n",
        "\n",
        "<tr>\n",
        "<td valign=\"top\" width=\"133\"><b>TEMPERATURE</b></td>\n",
        "<td valign=\"top\" width=\"133\"><b>Play = Yes</b></td>\n",
        "<td valign=\"top\" width=\"133\"><b>Play = No</b></td>\n",
        "<td valign=\"top\" width=\"133\"><b>Total</b></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td valign=\"top\" width=\"133\">Hot</td>\n",
        "<td valign=\"top\" width=\"133\">2/9</td>\n",
        "<td valign=\"top\" width=\"133\">2/5</td>\n",
        "<td valign=\"top\" width=\"133\">4/14</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td valign=\"top\" width=\"133\">Mild</td>\n",
        "<td valign=\"top\" width=\"133\">4/9</td>\n",
        "<td valign=\"top\" width=\"133\">2/5</td>\n",
        "<td valign=\"top\" width=\"133\">6/14</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td valign=\"top\" width=\"133\">Cool</td>\n",
        "<td valign=\"top\" width=\"133\">3/9</td>\n",
        "<td valign=\"top\" width=\"133\">1/5</td>\n",
        "<td valign=\"top\" width=\"133\">4/14</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "<br><br>\n",
        "<table width=\"400\" border=\"1\" cellspacing=\"0\" cellpadding=\"2\">\n",
        "<tbody>\n",
        "<tr>\n",
        "<td valign=\"top\" width=\"133\"><b>HUMIDITY</b></td>\n",
        "<td valign=\"top\" width=\"133\"><b>Play = Yes</b></td>\n",
        "<td valign=\"top\" width=\"133\"><b>Play = No</b></td>\n",
        "<td valign=\"top\" width=\"133\"><b>Total</b></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td valign=\"top\" width=\"133\">High</td>\n",
        "<td valign=\"top\" width=\"133\">3/9</td>\n",
        "<td valign=\"top\" width=\"133\">4/5</td>\n",
        "<td valign=\"top\" width=\"133\">7/14</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td valign=\"top\" width=\"133\">Normal</td>\n",
        "<td valign=\"top\" width=\"133\">6/9</td>\n",
        "<td valign=\"top\" width=\"133\">1/5</td>\n",
        "<td valign=\"top\" width=\"133\">7/14</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "<br><br>\n",
        "<table width=\"400\" border=\"1\" cellspacing=\"0\" cellpadding=\"2\">\n",
        "<tbody>\n",
        "<tr>\n",
        "<td valign=\"top\" width=\"133\"><b>WIND</b></td>\n",
        "<td valign=\"top\" width=\"133\"><b>Play = Yes</b></td>\n",
        "<td valign=\"top\" width=\"133\"><b>Play = No</b></td>\n",
        "<td valign=\"top\" width=\"133\"><b>Total</b></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td valign=\"top\" width=\"133\">Strong</td>\n",
        "<td valign=\"top\" width=\"133\">3/9</td>\n",
        "<td valign=\"top\" width=\"133\">3/5</td>\n",
        "<td valign=\"top\" width=\"133\">6/14</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td valign=\"top\" width=\"133\">Weak</td>\n",
        "<td valign=\"top\" width=\"133\">6/9</td>\n",
        "<td valign=\"top\" width=\"133\">2/5</td>\n",
        "<td valign=\"top\" width=\"133\">8/14</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBxNXDs8WcE8"
      },
      "source": [
        "2. Testing $\\rightarrow$ compute class for sample **X** <br>\n",
        "\n",
        "- formulas: $Class(X) = argmax_{c_j} \\Bigg\\{ \\left[ \\prod\\limits_{\\alpha=1}^m   P(\\color{red}{a_\\alpha^{(i)}}, C=c_j) \\right] \\times \\color{orange}{P(C=c_j)} \\Bigg\\}$\n",
        "- sample: ```X = (Outlook=Sunny, Temperature=Cool, Humidity=High, Wind=Strong)```\n",
        "- probabilities for class=<font color=\"green\">Yes</font>:\n",
        "<ul>\n",
        "<li>P(Outlook=Sunny | Play=<font color=\"green\">Yes</font>) = 2/9</li>\n",
        "<li>P(Temperature=Cool | Play=<font color=\"green\">Yes</font>) = 3/9</li>\n",
        "<li>P(Humidity=High | Play=<font color=\"green\">Yes</font>) = 3/9</li>\n",
        "<li>P(Wind=Strong | Play=<font color=\"green\">Yes</font>) = 3/9</li>\n",
        "<li>P(Play=<font color=\"green\">Yes</font>) = 9/14</li>\n",
        "</ul>\n",
        "\n",
        "- probabilities for class=<font color=\"red\">No</font>:\n",
        "<ul>\n",
        "<li>P(Outlook=Sunny | Play=<font color=\"red\">No</font>) = 3/5</li>\n",
        "<li>P(Temperature=Cool | Play=<font color=\"red\">No</font>) = 1/5</li>\n",
        "<li>P(Humidity=High | Play=<font color=\"red\">No</font>) = 4/5</li>\n",
        "<li>P(Wind=Strong | Play=<font color=\"red\">No</font>) = 3/5</li>\n",
        "<li>P(Play=<font color=\"red\">No</font>) = 5/14</li>\n",
        "</ul>\n",
        "<br>\n",
        "- P(X|Play=<font color=\"green\">Yes</font>)P(Play=<font color=\"green\">Yes</font>) = <br>\n",
        "= P(Outlook=Sunny | Play=<font color=\"green\">Yes</font>) * P(Temperature=Cool | Play=<font color=\"green\">Yes</font>) * P(Humidity=High | Play=<font color=\"green\">Yes</font>) * P(Wind=Strong | Play=<font color=\"green\">Yes</font>* P(Play=<font color=\"green\">Yes</font>)  \n",
        "= $\\frac{2}{9} * \\frac{3}{9} * \\frac{3}{9} * \\frac{3}{9} * \\frac{9}{14}$ = 0.0053\n",
        "<br><br>\n",
        "- P(X|Play=<font color=\"red\">No</font>)P(Play=<font color=\"red\">No</font>) = \\\\\n",
        "= P(Outlook=Sunny | Play=<font color=\"red\">No</font>) * P(Temperature=Cool | Play=<font color=\"red\">No</font>) * P(Humidity=High | Play=<font color=\"red\">No</font>) * P(Wind=Strong | Play=<font color=\"red\">No</font>) * P(Play=<font color=\"red\">No</font>) \n",
        "= $\\frac{3}{5} * \\frac{1}{5} * \\frac{4}{5} * \\frac{3}{5} * \\frac{5}{14} = 0.0206$\n",
        "<br><br>\n",
        "- P(X) = P(Outlook=Sunny) * P(Temperature=Cool) * P(Humidity=High) * P(Wind=Strong) =  $\\frac{5}{14} * \\frac{4}{14} * \\frac{7}{14} * \\frac{6}{14} = 0.02186$\n",
        "\n",
        "- P(Play=<font color=\"green\">Yes</font>|X) = $\\frac{P(X|Play=\\color{green}{Yes}) * P(Play=\\color{green}{Yes})}{P(X) }$ = $\\frac{0.0053}{0.02186}$ = 0.24\n",
        "\n",
        "- P(Play=<font color=\"red\">No</font>|X) = $\\frac{P(X|Play=\\color{red}{No}) * P(Play=\\color{red}{No})}{P(X) }$ = $\\frac{0.0206}{0.02186}$ = 0.94\n",
        "\n",
        "- Since 0.9421 is greater than 0.2424 the class will be **No**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjFqMYQzVPdg"
      },
      "source": [
        "## Smoothing\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1XBorHfwK_1oHyB18l-4NmiV252ES4mY3\"></img>\n",
        "\n",
        "Imagine that you flip a coin and you get HEADs 100 times in a row. Then, the odds are: P(HEAD)=100% and P(TAIL)=0%. Supposing that you have a fair coin, it would not be really true that you never get a TAIL at a toss. \n",
        "\n",
        "If you try to compute the class for this sample: ```X = (Outlook=Overcast, Temperature=Cool, Humidity=High, Wind=Strong)```, you will see that the value of Naive Bayes formulas for class <font color=\"red\">No</font> would be 0 thanks to P(Outlook=Overcast|Play=No).\n",
        "\n",
        "In order to solve this problem, we may need the help of probability smoothing technique which `is a language modeling technique that assigns some non-zero probability to events that were unseen in the training data. This has the effect that the probability mass is divided over more events, hence the probability distribution becomes more smooth.` \n",
        "\n",
        "$$P(A_i=a_i^{(i)}|C=c_j) = \\frac{n_{ij}+\\lambda}{n_j+\\lambda*m_i}$$\n",
        "\n",
        "- $n_{ij}$ - number of samples that have both $A_i=a_i^{(i)}$ and $C=c_j$\n",
        "- $n_j$ - total number of samples that have $C=c_j$\n",
        "- $m_i$ - number of distinct values for attribute $A_i$\n",
        "- $\\lambda$ - a value between [0,1], if it is 0, the value remains unchanged, in our case we need it set to $\\frac{1}{n}$, where n is the number of samples\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9ugQeR8WdSl"
      },
      "source": [
        "## Observations\n",
        "\n",
        "- The Naive Bayes classifier presented in this lab works only with categorical data. If data is continous it would be hard to compute probabilities.  As the number of features increase linearly, the amount of training data required for classification increases exponentially. You have the followin option: you either transform numerical values to categorical data or you use a PDF (probability density function).\n",
        "\n",
        "$$f(x) = \\frac{1}{\\sqrt{2*\\pi}*\\sigma}*e^{-\\frac{(x-\\mu)^2}{2*\\sigma^2}}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "| Play Golf | Humidity   | Mean | Std |\n",
        "|------|------|------|------|\n",
        "|Yes | 86\t96\t80\t65\t70\t80\t70\t90\t75 | 79.1 | 10.2 |\n",
        "|No  | 85\t90\t70\t95\t91 | 86.2 | 9.7 |\n",
        "\n",
        "<br>\n",
        "\n",
        "$$P(Humidity=74 | Play=Yes) = \\frac{1}{\\sqrt{2*\\pi}*10.2}*e^{-\\frac{(74-79.1)^2}{2*10.2^2}} = 0.0344$$\n",
        "\n",
        "$$P(Humidity=74 | Play=Yes) = \\frac{1}{\\sqrt{2*\\pi}*9.7}*e^{-\\frac{(74-86.2)^2}{2*9.7^2}}=0.0187$$\n",
        "\n",
        "<br>\n",
        "\n",
        "- Nice to read: <a href=\"https://math.stackexchange.com/questions/469974/why-would-i-use-bayes-theorem-if-i-can-directly-compute-the-posterior-probabili\">Why would I use Bayes' Theorem if I can directly compute the posterior probability?</a> \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQKEjOaDuWuX"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv1MdGCYulRb"
      },
      "source": [
        "## E1. Naive Bayes\n",
        "\n",
        "Implement a classifier using the formulas from **Naive Bayes Classifier** section on **Play Tennis** dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--EPQgWoBRU6"
      },
      "source": [
        "### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hfQQv6qBd0k"
      },
      "source": [
        "!wget -O play_tennis.csv \"https://drive.google.com/uc?export=download&id=1NT1iJNj3HrPNtiLCb-myrY0XHaJ8_jtf\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEUB9SA_BiKg"
      },
      "source": [
        "### Process Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VILLaebOBkEw"
      },
      "source": [
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "\n",
        "def train_test_split(df, train_percent=.85):\n",
        "  no_samples, _ = df.shape\n",
        "  no_train_samples = int(0.85 * no_samples)\n",
        "  no_test_samples = no_samples - no_train_samples\n",
        "  train_df = tennis_df.iloc[:no_train_samples, :]\n",
        "  test_df = tennis_df.iloc[-no_test_samples:, :]\n",
        "\n",
        "  X_train = train_df.iloc[:, :-1]\n",
        "  y_train = train_df.iloc[:, -1]\n",
        "  X_test = test_df.iloc[:, :-1]\n",
        "  y_test = test_df.iloc[:, -1]\n",
        "  return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "\n",
        "tennis_df = pd.read_csv(\"play_tennis.csv\", header=0)  \n",
        "\n",
        "#this column is dropped because it brings no meaningful information (it's only a day id)\n",
        "del tennis_df[\"day\"]\n",
        "\n",
        "print(\"Dataset before split\")\n",
        "display(HTML(tennis_df.to_html()))\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tennis_df)\n",
        "\n",
        "print(\"Dataset used for training\")\n",
        "display(HTML(X_train.to_html()))\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "print(\"Dataset used for testing\")\n",
        "display(HTML(X_test.to_html()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvtLYCneB1D6"
      },
      "source": [
        "### Implement Classifier\n",
        "\n",
        "* Tasks for **self.fit** method\n",
        "\n",
        "  * **TODO 1** - compute `self.count_classes`\n",
        "\n",
        "  * **TODO 2** - compute `self.prob_classes`\n",
        "\n",
        "  * **TODO 3** - compute `self.count_values`\n",
        "\n",
        "  * **TODO 4** - compute `self.prob_values`\n",
        "\n",
        "  * **TODO 5** - smooth probabilities\n",
        "\n",
        "* Task for **self.predict** method:\n",
        "  * **TODO 6** - compute probability of each class for each sample\n",
        "  * **TODO 7** - predict the class with the maximum probability\n",
        "\n",
        "  * $Class(X) = argmax_{c_j} \\Bigg\\{ \\left[ \\prod\\limits_{\\alpha=1}^m   P(\\color{red}{a_\\alpha^{(i)}}, C=c_j) \\right] \\times \\color{orange}{P(C=c_j)} \\Bigg\\}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO8WKcfoB3EK"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class NaiveBayesClassifier:  \n",
        "  \n",
        "  #class constructor\n",
        "  def __init__(self):\n",
        "    # dictionary to store the counts of a class\n",
        "    # e.g. {'No': 5, 'Yes': 9}\n",
        "    self.count_classes = dict()\n",
        "\n",
        "    # dictionary to store the probability of a class\n",
        "    # e.g. {'No': 0.35714285714285715, 'Yes': 0.6428571428571429}\n",
        "    self.prob_classes = dict()\n",
        "\n",
        "    # dictionary to store the counts of attribute value A given class C\n",
        "    # e.g. {('Overcast', 'No'): 0, ..., ('Mild', 'Yes'): 4}\n",
        "    self.count_values = dict()\n",
        "\n",
        "    # dictionary to store the probability of each attribute value A given class C\n",
        "    # e.g. {('Hot', 'No'): 0.4, ..., ('Sunny', 'Yes'): 0.2222222222222222}\n",
        "    self.prob_values = dict()\n",
        "\n",
        "  # train method of Naive Bayes Classifier\n",
        "  def fit(self, x, y):\n",
        "\n",
        "    # [TODO 1,2] compute the count and probability for each class\n",
        "    print(x)\n",
        "    print(y)\n",
        "    val, counts = np.unique(y, return_counts=True)\n",
        "    #print(val)\n",
        "    #print(counts)\n",
        "    #1\n",
        "    for (v, c) in zip(val, counts):\n",
        "      self.count_classes[v] = c\n",
        "    print(self.count_classes)\n",
        "\n",
        "    #2\n",
        "    no_classes = len(y)\n",
        "    for (v, c) in self.count_classes.items():\n",
        "      self.prob_classes[v] = c / no_classes\n",
        "    print(self.prob_classes)\n",
        "    \n",
        "    # [TODO 3,4] compute the count and probability for each attribute's value in the datasat given class C\n",
        "    flatl = flat_list = [attr for l in x for attr in l]\n",
        "    flatlunique = list(dict.fromkeys(flatl))\n",
        "    #rows = {}\n",
        "    #for (l, cls) in zip(x, y):\n",
        "    #  rows[np.where(x == l)[0][0]] = cls\n",
        "    lbd = 1\n",
        "    for attr in flatlunique:\n",
        "      self.prob_values[(attr, 'Yes')] = 0\n",
        "      self.prob_values[(attr, 'No')] = 0\n",
        "      #self.prob_values[(attr, 'Yes')] = len([attr for l in x if attr in l and rows[np.where(x == l)[0][0]] == 'Yes']) / self.count_classes['Yes']\n",
        "      #self.prob_values[(attr, 'No')] = len([attr for l in x if attr in l and rows[np.where(x == l)[0][0]] == 'No']) / self.count_classes['No']\n",
        "      attr_yes = 0\n",
        "      attr_no  = 0\n",
        "      for i in range(len(x)):\n",
        "        if attr in x[i]:\n",
        "          self.prob_values[(attr, y[i])] += 1\n",
        "      self.prob_values[(attr, 'Yes')] = (self.prob_values[(attr, 'Yes')] + lbd) / (self.count_classes['Yes'] + len(flatlunique) * lbd)\n",
        "      self.prob_values[(attr, 'No')]  = (self.prob_values[(attr, 'No')] + lbd) / (self.count_classes['No'] + len(flatlunique) * lbd) #self.count_classes['No']\n",
        "      \n",
        "          \n",
        "    print(self.prob_values)\n",
        "    # [TODO 5] apply smoothing on prob_values\n",
        "    \n",
        "  #test method of Naive Bayes Classifier\n",
        "  def predict(self, X):\n",
        "    # print sample -> target value (e.g ['Sunny' 'Hot' 'High' 'Strong'] -> No)\n",
        "    \n",
        "    #FOREACH sample\n",
        "    # [TODO 6] compute probability of each class\n",
        "    class_probs = dict()\n",
        "    class_probs = dict()\n",
        "    for i in range(len(X)):\n",
        "    #for x in X:\n",
        "      class_probs[(i, 'Yes')] = 1\n",
        "      class_probs[(i, 'No')] = 1\n",
        "      for attr in X[i]:\n",
        "        class_probs[(i, 'Yes')] *= self.prob_values[(attr, 'Yes')]\n",
        "        class_probs[(i, 'No')]  *= self.prob_values[(attr, 'No')]\n",
        "      class_probs[(i, 'Yes')] *= self.prob_classes['Yes']\n",
        "      class_probs[(i, 'No')]  *= self.prob_classes['No']\n",
        "    # [TODO 7] compute argmax on class_probs\n",
        "    for i in range(len(X)):\n",
        "    #for x in X:\n",
        "      PX = 1\n",
        "      for attr in X[i]:\n",
        "        PX *= (self.prob_values[(attr, 'Yes')] + self.prob_values[(attr, 'No')])\n",
        "      pyes = class_probs[(i, 'Yes')] / PX\n",
        "      pno  = class_probs[(i, 'No')] / PX\n",
        "      if pyes > pno:\n",
        "        print(str(X[i]) + ' --> ' + 'Yes' + str(pyes) + str(pno))\n",
        "      else:\n",
        "        print(str(X[i]) + ' --> ' + 'No' + str(pno) + str(pyes))\n",
        "        \n",
        "\n",
        "naive = NaiveBayesClassifier()\n",
        "naive.fit(X_train.values, y_train.values)\n",
        "naive.predict(X_test.values)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}