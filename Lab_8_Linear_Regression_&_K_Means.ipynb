{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7458PF8eKp23"
      },
      "source": [
        "# Regression\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_6Xloxhdb5f"
      },
      "source": [
        "Regression is a statistical measurement used in multiple fields that attempts to determine the realationship between a dependent variable (called also output variable or response) and a series of independent variables (called also features, covariates, predictors). This chapter presents the parametric model.\n",
        "\n",
        "\n",
        "- predicting house price based on the number of rooms\n",
        "- predicting credit card score (credit score is a three digit number that summarizes how well a person or business has handled debt)\n",
        "- predicting a company's sales based on the amount of money invested in advertising\n",
        "\n",
        "All parametric regression models take the following form:\n",
        "\n",
        "$\\large \\color{red}{y} = \\color{green}{f(x)} + \\color{blue}{\\epsilon} $\n",
        "\n",
        "- $\\large \\color{red}{y}$ - predicted output\n",
        "- $\\color{green}{f(x)}$ - unknown function we want to find\n",
        "- $\\Large \\color{blue}{\\epsilon}$ - error term (this error is called also irreducible error, even if we can find a perfect regression model we will still have this error)\n",
        "\n",
        "We are interested in finding the unknown function  $\\color{green}{f(x)}$. This function depends on the regression type.\n",
        "\n",
        "- Simple Linear Regression: $f(x) = \\beta_0 + \\beta_1 * x$\n",
        "- Multiple Linear Regression: $f(x) = \\beta_0 + \\beta_1 * x_1 + \\beta_2 * x_2 + \\cdots  + \\beta_n * x_n$\n",
        "\n",
        "The goal is to minimize: $e_i=y_i-\\hat{y_i}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XInXWiz0huet"
      },
      "source": [
        "## Simple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idxGAU2Zlg5K"
      },
      "source": [
        "### Mathematical Proof"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-BqjZOMgAsO"
      },
      "source": [
        "\n",
        "\n",
        "Simple Linear Regression's objective is to determine the value of coefficients $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>Linear Regression Estimate</b>\n",
        "\n",
        "$\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}*x$\n",
        "\n",
        "<br>\n",
        "\n",
        "<b> Linear Regression Actual Response </b>\n",
        "\n",
        "$y = f(X) + \\epsilon$\n",
        "\n",
        "We need to minimize the sum of the squared prediction errors (there are certain reason to use the squared error and not the absolute value, for example it penalizes  more larger errors, it is a differentiable function and the \"distance\" between the true value and the estimated value is always positive):\n",
        "\n",
        "$S(\\hat{\\beta_0}, \\hat{\\beta_1}) = {argmin}_{\\hat{\\beta_0}, \\hat{\\beta_1}}{\\sum\\limits_{i=1}^{n}e_i^2} = {argmin}_{\\hat{\\beta_0}, \\hat{\\beta_1}} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i)^2$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "$\\frac{\\partial S(\\hat{\\beta_0}, \\hat{\\beta_1})}{\\partial \\hat{\\beta_0}} = 0 \\\\ \\Rightarrow \\frac{\\partial\\Big(\\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i)^2\\Big)}{\\partial \\hat{\\beta_0}} = 0 \\\\ \n",
        "\\Rightarrow -2\\Big(\\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i)\\Big) = 0 \\\\ \n",
        "\\Rightarrow \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i) = 0 \\\\ \n",
        "\\Rightarrow n\\beta_0 = \\sum\\limits_{i=1}^{n}(y_i- \\hat{\\beta_1}x_i) \\\\ \\Rightarrow \\hat{\\beta_0} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}y_i - \\frac{1}{n}\\sum\\limits_{i=1}^{n}\\hat{\\beta_1}x_i  \\\\\n",
        "\\Rightarrow \\color{red}{\\hat{\\beta_0} =  \\bar{y} - \\hat{\\beta_1}\\bar{x}}\n",
        "$\n",
        "\n",
        "---\n",
        "\n",
        "$\\frac{\\partial S(\\hat{\\beta_0}, \\hat{\\beta_1})}{\\partial \\hat{\\beta_1}} = 0 \\\\ \\Rightarrow \\frac{\\partial\\Big(\\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta_0} - \\hat{\\beta_1}x_i)^2\\Big)}{\\partial \\hat{\\beta_1}} = 0 \\\\\n",
        "\\Rightarrow -2\\Big(\\sum\\limits_{i=1}^{n}(y_i - \\color{red}{\\hat{\\beta_0}} - \\hat{\\beta_1}x_i)x_i\\Big) = 0 \\\\\n",
        "\\Rightarrow -2\\Big(\\sum\\limits_{i=1}^{n}(y_i - \\color{red}{(\\bar{y} - \\hat{\\beta_1}\\bar{x})} - \\hat{\\beta_1}x_i)x_i\\Big) = 0 \\\\\n",
        "\\Rightarrow -2\\Big(\\sum\\limits_{i=1}^{n}(y_i - \\bar{y} + \\hat{\\beta_1}(x_i - \\bar{x}))x_i\\Big) = 0 \\\\\n",
        " \\Rightarrow \\sum\\limits_{i=1}^{n}(y_i - \\bar{y} + \\hat{\\beta_1}(x_i - \\bar{x})) = 0 \\\\\n",
        " \\Rightarrow  \\sum\\limits_{i=1}^{n}(y_i - \\bar{y}) = \\sum\\limits_{i=1}^{n}\\hat{\\beta_1}(x_i - \\bar{x}) \\\\\n",
        " \\Rightarrow \\hat{\\beta_1} = \\frac{\\sum\\limits_{i=1}^{n}(y_i - \\bar{y})}{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})} \\\\ \n",
        "\\Rightarrow  \\color{green}{\\hat{\\beta_1} = \\frac{\\sum\\limits_{i=1}^{n}(y_i - \\bar{y})(x_i - \\bar{x})}{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}}\n",
        "$\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEKJUKt4hHs-"
      },
      "source": [
        "![](http://drive.google.com/uc?export=view&id=1-OWE72vwxALCzcwNRAfMbFe0QGZXGqHK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtYC0hVYllqW"
      },
      "source": [
        "### Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek8vUdnAcn9u"
      },
      "source": [
        "\n",
        "The next example presents the dataset generation suitable for simple linear regression. Based on X variable (only one attribute) we want to predict the output variable y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oilnIMNNb6EF"
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "# generate regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10, bias=1, random_state=0)\n",
        "\n",
        "# plot regression dataset\n",
        "plt.scatter(X, y, cmap='winter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MlL7u8mBF5Q"
      },
      "source": [
        "### Ex0. SLR Implementation\n",
        "\n",
        "- compute $\\hat{\\beta_1}$ and $\\hat{\\beta_0}$\n",
        "  - $\\color{green}{\\hat{\\beta_1} = \\frac{\\sum\\limits_{i=1}^{n}(y_i - \\bar{y})(x_i - \\bar{x})}{\\sum\\limits_{i=1}^{n}(x_i - \\bar{x})^2}}$\n",
        "  - $\\color{red}{\\hat{\\beta_0} =  \\bar{y} - \\hat{\\beta_1}\\bar{x}}$\n",
        "- plot the data from *X* and *y* (use plt.scatter)\n",
        "- plot the line using SLR formulas $\\color{blue}{y=\\beta_0 + \\beta_1 x}$ (use plt.plot)\n",
        "- you can test your results with scikit-learn [implementation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq3QwcZndDqt"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# [TODO] SLR\n",
        "\n",
        "numerator = [(y[i] - np.mean(y)) * (X[i] - np.mean(X)) for i in range(len(X))]\n",
        "denominator = [np.power(X[i] - np.mean(X), 2) for i in range(len(X))]\n",
        "\n",
        "beta1 = sum(numerator) / sum(denominator)\n",
        "beta0 = np.mean(y) - beta1 * np.mean(X)\n",
        "y_pred = beta0 + beta1 * X\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, y_pred, '-r', label='line')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHbYJ-mKgEou"
      },
      "source": [
        "## Multiple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13xJ-jMRwDKD"
      },
      "source": [
        "### Mathematical Proof\n",
        "\n",
        "$ y = \n",
        "\\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_n\n",
        "\\end{bmatrix}\n",
        "$  - target vector\n",
        "\n",
        "$ X = \n",
        "\\begin{bmatrix}\n",
        "1 & X_{1,1} & X_{1,2} & \\cdots & X_{1,p} \\\\\n",
        "1 & X_{2,1} & X_{2,2} & \\cdots & X_{2,p} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\cdots & \\vdots\\\\\n",
        "1 & X_{n,1} & X_{n,2} & \\cdots & X_{n,p} \\\\\n",
        "\\end{bmatrix}\n",
        "$  - input matrix ( we put 1 on first columns which will correspond to the bias $\\beta_0$\n",
        "\n",
        "$ \\beta = \n",
        "\\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\vdots \\\\\n",
        "\\beta_p\n",
        "\\end{bmatrix}\n",
        "$ - coefficient vector\n",
        "\n",
        "$ \\epsilon = \n",
        "\\begin{bmatrix}\n",
        "\\epsilon_0 \\\\\n",
        "\\epsilon_1 \\\\\n",
        "\\vdots \\\\\n",
        "\\epsilon_p\n",
        "\\end{bmatrix}\n",
        "$ - error vector\n",
        "\n",
        "<b>Matrix Differentiation Formulas</b>\n",
        "\n",
        "- $y = A \\Rightarrow \\frac{\\partial y}{\\partial x} = 0$\n",
        "- $y = Ax \\Rightarrow \\frac{\\partial y}{\\partial x} = A$\n",
        "- $y = xA \\Rightarrow \\frac{\\partial y}{\\partial x} = A^T$\n",
        "- $y = x^T A x \\Rightarrow \\frac{\\partial y}{\\partial x} = 2 x^TA$\n",
        "<br><br>\n",
        "\n",
        "<b>Linear Regression Estimate</b>\n",
        "\n",
        "$\\hat{y} = X\\hat{\\beta}$\n",
        "\n",
        "<br>\n",
        "\n",
        "<b> Linear Regression Actual Response </b>\n",
        "\n",
        "$y = f(X) + \\epsilon$\n",
        "\n",
        "Again, we need to minimize RSS\n",
        "\n",
        "$RSS = {\\sum\\limits_{i=1}^{n}e_i^2}$\n",
        "\n",
        "Because we are speaking in term of vectors we can replace the sum with the dot product between vectors.\n",
        "\n",
        "<br>\n",
        "\n",
        "$RSS = e^T e \\\\\n",
        "\\Rightarrow (y-\\hat{y})^T(y-\\hat{y}) \\\\\n",
        "\\Rightarrow (y-X\\hat{\\beta})^T(y-X\\hat{\\beta}) \\\\\n",
        "\\Rightarrow (y^T-\\hat{\\beta^T} X^T)(y-X\\hat{\\beta}) \\\\\n",
        "\\Rightarrow y^Ty-y^T X \\hat{\\beta^T} - \\hat{\\beta} X^T y + \\hat{\\beta^T} X^T X \\hat{\\beta}\n",
        "$\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\frac{\\partial(RSS)}{\\partial \\hat{\\beta}} = 0 \\\\\n",
        "\\Rightarrow \\frac{ \\partial(y^Ty-y^T X \\hat{\\beta^T} - \\hat{\\beta} X^T y + \\hat{\\beta^T} X^T X)}{\\partial \\hat{\\beta}} = 0\\\\\n",
        "\\Rightarrow \\frac{\\partial (y^Ty)}{\\partial \\hat{\\beta}} - \\frac{\\partial (y^T X \\hat{\\beta^T)} }{\\partial \\hat{\\beta}} - \\frac{\\partial (\\hat{\\beta} X^T y)}{\\partial \\hat{\\beta}} + \\frac{\\partial (\\hat{\\beta^T} X^T X)}{\\partial \\hat{\\beta}} = 0 \\\\\n",
        "\\Rightarrow 0 - y^T X - (X^T y)^T + 2 \\hat{\\beta^T} X^T X = 0 \\\\\n",
        "\\Rightarrow 0 - y^T X - y^T X + 2 \\hat{\\beta^T} X^T X = 0 \\\\\n",
        "\\Rightarrow 2 \\hat{\\beta^T} X^T X = 2y^T X \\\\\n",
        "\\Rightarrow \\hat{\\beta^T} = y^T X (X^T X)^{-1} \\\\\n",
        "\\Rightarrow \\color{blue}{\\hat{\\beta} = (X^TX)^{-1}X^Ty}\n",
        "$\n",
        "\n",
        "<br>\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1Qg08PhNF5CiHEkeQC1iViopm0kx31_pK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ToDTvy-wHjt"
      },
      "source": [
        "### Ex 1. MLR Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG-gpstfKnxD"
      },
      "source": [
        "- compute $\\hat{\\beta}$\n",
        "  - $\\color{blue}{\\hat{\\beta} = (X^TX)^{-1}X^Ty}$\n",
        "- add a column full of ones at the beggining of *X*\n",
        "- you can test your results with scikit-learn [implementation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDYDLGUeg9-c"
      },
      "source": [
        "!wget -O advertising.csv \"https://drive.google.com/uc?export=download&id=1vhzMbpyZlFPSJdmA7gM4bs4I5AOjFng4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-aVivAHl3hq"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "import pandas as pd\n",
        "\n",
        "ads_df = pd.read_csv(\"advertising.csv\")\n",
        "\n",
        "# drop first column, it contains only line index\n",
        "ads_df = ads_df.drop(['Unnamed: 0'], axis=1)\n",
        "\n",
        "# [TODO] add a column full of 1s\n",
        "col = np.ones(ads_df.shape[0])\n",
        "ads_df.insert(0, 'Ones', col)\n",
        "ads_df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ILx3J-Sng0q"
      },
      "source": [
        "from numpy.linalg import inv\n",
        "from numpy.linalg import multi_dot\n",
        "import numpy as np\n",
        "\n",
        "# [TODO] mlr\n",
        "X = ads_df.drop('Sales', axis=1)\n",
        "y = ads_df.iloc[:,-1]\n",
        "\n",
        "#print(X)\n",
        "#print(y)\n",
        "\n",
        "transp = np.transpose(X)\n",
        "beta = multi_dot([np.linalg.inv(np.dot(transp, X)), transp, y])\n",
        "beta0 = beta[0]\n",
        "beta1 = beta[1]\n",
        "\n",
        "y_pred = np.dot(X,beta)\n",
        "print(y)\n",
        "print(y_pred)\n",
        "\n",
        "\n",
        "#plt.plot(X, y_pred, '-r', label='line')\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_m0_-lQg44_"
      },
      "source": [
        "## Note\n",
        "\n",
        "In this lab, the closed-form solution for estimating regression's parameteres is presented for both models due to lack of time. If we have to deal with a large dataset, it becomes expensive to compute the inverse of a matrix (there might be cases where we can not load the whole dataset in the RAM). To solve this problem, there are iterative approaches that uses gradient descent (or other optimization techniques) to determine the parameters ([here](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931) is a good start to read about this)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haKW7ajILADt"
      },
      "source": [
        "# K-Means\n",
        "\n",
        "`K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqM9-3K-QEtz"
      },
      "source": [
        "## Pseudocode\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1amAJlJAmEHUy45mw-Ix0ydx25Z30GM_Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jemDxaBOL8wX"
      },
      "source": [
        "## Distance \n",
        "\n",
        "The distance between points uses the euclidian distance (or generalized L2-norm). From Pythagoras' theorem we can derive that the distance between two points $(x_1,y_1)$ and $(x_2,y_2)$ is $d=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$\n",
        "\n",
        "\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1xBP83iAPbF5yfoGHHG4j2pX9wXAcys0U)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXZfGLTVQNXe"
      },
      "source": [
        "## Example\n",
        "\n",
        "### Dataset\n",
        "\n",
        "We receive a dataset consisting of 15 points without prior knowledge about their class. Now, we want to group those points. After plotting the data we can see which are the groups, without computing any complicated mathematical formulas. The problem arises when the space becomes higher than 3-dimensions or when the space is non-euclidian. \n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1lVHrf6HlsS64XIeSEYoDjwakx1DjPyO_)\n",
        "\n",
        "\n",
        "### Step 1\n",
        "\n",
        "Suppose we want to create three groups, then centroids are initialized with three random points ${C_1, C_2, C_3}$ (not so random in this picture, but for the sake of illustrations...). This example will use euclidian distance.\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1yfGKXqG5mtFyfFZrfEumbgB-Pqgxxec_)\n",
        "\n",
        "\n",
        "### Step 2\n",
        "\n",
        "In this step, we take each point in the dataset and assign it to the most closest centroid (e.g points A and B will be close to centroid $C_1$)\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1gJZ9-vuQaIbLVFwmHmDXmOmRFgjMqRU5)\n",
        "\n",
        "\n",
        "### Step 3\n",
        "After assigning each point to a cluster, we have this color map. We can see an outlier in our blue cluster.\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=11HWKVF1ZD-c7HqOFOFBuTJ0yMNbWCkZp)\n",
        "\n",
        "\n",
        "### Step 4\n",
        "\n",
        "Based on the clusters formed at previous step, we may now recompute the new centroids ${C_1', C_2', C_3'}$.\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1Qj5p1gGPQccjTrzl3MYRZ0plkKnGd3pC)\n",
        "\n",
        "\n",
        "### Step 5\n",
        "Finally, the outlier will come back to its belonging group $C_3'$. Go back to **Step 3** until stopping criteria is met.\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1H0-C1hLH5BBMM8GlaI1hUGgAesiiG882)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQuRzzphYIlp"
      },
      "source": [
        "## Number of Clusters\n",
        "\n",
        "One primary and significant drawback of k-Means is that it requires prior knowledge or assumption about number of clusters. WCSS (Within Cluster Sum of Squares) comes to help us by  measuring the squared average distance of all the points within a cluster to the cluster centroid\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=11sZziVpVbavowtTtOQNM-jZlOz5n3a2c)\n",
        "\n",
        "The optimal number of clusters would be the longest distance to the straight line from the points on the curve. \n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1UNPFQyekw4hKbmBf10dRbwFcLAsasNiM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pccEMpLdRUBQ"
      },
      "source": [
        "## Ex 2. KMeans Implementation\n",
        "\n",
        "Implement KMeans based on the following dataset.\n",
        "Compute train and test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjhHTr03MEW3"
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "\n",
        "X, _ = make_blobs(n_samples=100, centers=3, n_features=2, random_state=2, cluster_std=.6)\n",
        "plt.scatter(X[:,0], X[:,1], cmap='winter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HloD9xnaWpHG"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "wcss = []\n",
        "\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters = i,  max_iter = 10, random_state = 0)\n",
        "    kmeans.fit(X)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "    \n",
        "#Plotting the results onto a line graph, allowing us to observe 'The elbow'\n",
        "plt.plot(range(1, 11), wcss)\n",
        "plt.title('The elbow method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS') #within cluster sum of squares\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chKu4H4lLUWi"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "class KMeans():\n",
        "  def __init__(self, n_clusters=3, init='random', max_iter=10):\n",
        "    self.n_clusters = n_clusters\n",
        "    self.max_iter = max_iter\n",
        "\n",
        "  def fit(self, X):\n",
        "    # Task 1. Select initial centroids\n",
        "    no_samples, no_attrs = X.shape\n",
        "    np.random.seed(4)\n",
        "    rand_index = np.random.choice(no_samples, self.n_clusters, replace=False)\n",
        "    self.centroids = X[rand_index]\n",
        "    self.cluster_parent = np.zeros(no_samples)\n",
        "    plt.scatter(X[:,0], X[:,1], c=self.cluster_parent)\n",
        "\n",
        "    for iter in range(self.max_iter):\n",
        "\n",
        "      # Task 1. compute points' distances from centroid, \n",
        "      #         assign the points to the closest cluster\n",
        "      # [TODO]\n",
        "      print(self.centroids.shape[0])\n",
        "      clusters = [[], [], []]\n",
        "      i = 0\n",
        "      for p in X:\n",
        "        #min_d = float('inf')\n",
        "        distances = [np.linalg.norm(p-c) for c in self.centroids]\n",
        "        min_d = min(distances)\n",
        "        min_ind = distances.index(min_d)\n",
        "        clusters[min_ind].append(p)\n",
        "        self.cluster_parent[i] = min_ind\n",
        "        i = i + 1\n",
        "        #for index in range(len(centroids)):\n",
        "        #  d = numpy.linalg.norm(p-centroids(index))\n",
        "        #  if d < min_d:\n",
        "        #    min_d = d\n",
        "        #    centroid_index = index\n",
        "\n",
        "      \n",
        "      # Task 2. compute the new centroids\n",
        "      # [TODO]\n",
        "      for ind in range(self.centroids.shape[0]):\n",
        "        c_ind = np.random.choice(len(clusters[ind]), 1, replace=False)\n",
        "        self.centroids[ind] = clusters[ind][c_ind[0]]\n",
        "\n",
        "      print(self.cluster_parent.shape)\n",
        "      \n",
        "     # flat_list = [item for sublist in clusters for item in sublist]\n",
        "     #print(len(flat_list))\n",
        "      \n",
        "      #for j in range(self.centroids.shape[0]):\n",
        "        #self.cluster_parent = np.asarray(flat_list)\n",
        "        #self.cluster_parent[j] = clusters[j]\n",
        "      \n",
        "      \n",
        "      #self.cluster_parent = flat_list\n",
        "      #plot only if 2D points\n",
        "      if no_attrs == 2:\n",
        "        plt.scatter(X[:,0], X[:,1], c=self.cluster_parent, cmap='winter')\n",
        "        plt.scatter(self.centroids[:,0], self.centroids[:,1], \n",
        "                    c=range(self.n_clusters), marker='*', s=900, cmap='winter')\n",
        "        plt.pause(0.05)\n",
        "\n",
        "  def predict(self, X):\n",
        "    pass\n",
        "\n",
        "kMeansClf = KMeans()\n",
        "kMeansClf.fit(X)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}