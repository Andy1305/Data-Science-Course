{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lun4kDh898F3"
      },
      "source": [
        "# Resampling Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rEIgFV3--bf"
      },
      "source": [
        "## Cross-Validation\n",
        "\n",
        "Cross-Validation (or K-Fold Cross-Validation) involves randomly partioning the original dataset in K datasets. All subsets are disjoint (two datasets are disjoint datasets if they do have no sample/observation in common). The motivation appears when the dataset has a small number of samples and we would like to check all possible ways whether the model has been overfitted or not.\n",
        "\n",
        "```\n",
        "Step 1. Train on folds {2,3,4,5} and test on fold 1\n",
        "...\n",
        "Step 5. Train of folds {1,2,3,4} and test on fold 5\n",
        "```\n",
        "\n",
        "<br>\n",
        "Usually, accuracy is computed as the average of the accuracy for each fold.\n",
        "<br><br>\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1vEpRsWfG6BVEeWTq0HWnB4rjYd80Aiwz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpP-4HA49-0y"
      },
      "source": [
        "## Bootstrap\n",
        "\n",
        "Bootstrap is a machine learning technique that relies on random sampling with replacement (a samples can be selected multiple times)\n",
        "\n",
        "```\n",
        "- Choose a number of bootstrap samples to perform\n",
        "- Choose a bootsrap sample size\n",
        "- For each bootstrap sample\n",
        "  - Draw a sample with replacement with the chosen size\n",
        "  - Fit a model on the data sample\n",
        "  - Estimate the skill of the model on the out-of-bag sample.\n",
        "  - Calculate the mean of the sample of model skill estimates.\n",
        "```\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1UbMxQfvtt6HHWHQq3w10ns1QPJk-quRq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8o6os3Tlfbo"
      },
      "source": [
        "# Bagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxhAiVG-sx-_"
      },
      "source": [
        "\n",
        "\n",
        "Ensemble methods represent a machine learning technique which combines several models in order to obtain a better predictive model. Ensemble methods require much more computation than a single model, thus it arises the problem of space and time vs optimal learning tradeoff. Each base model can be created using different subsets of the same training dataset and same algorithm, or using the same dataset with different algorithms, or any other method.\n",
        "\n",
        "Bagging:\n",
        "- BAGGing combines Bootstrapping and Aggregation to form an ensamble model.\n",
        "- Bagging steps:\n",
        "  - Starting with the original dataset, build ùíè training datasets by sampling with replacement (bootstrap samples)\n",
        "  - For each training dataset build a classifier using the same learning algorithm.\n",
        "  - The final classifier is obtained by combining the results of\n",
        "each classifiers (by voting for example).\n",
        "- Bagging helps to improve the accuracy for unstable learning algorithms: decision trees, neural networks. \n",
        "- It does not help for kNN, Na√Øve Bayesian classification or CARs.\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1kUu7OwAuw-Cx0c3_GmOp-WXPdyu1QEci)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coUv9K5_s4uH"
      },
      "source": [
        "## Bagging Forest vs Random Forest\n",
        "\n",
        "- final classifier outputs\n",
        "  - classification: the modal value (most frequent value)\n",
        "  - regression: the average over all predictions\n",
        "- Bagging Forest computes the output by using an ensemble of Decision Tree classifiers\n",
        "- Random Forest use also Decision Tree classifiers, but it brings an improvement of the greedy split method used by classical Decision Tree. In CART, when selecting a split node, the learning algorithm is allowed to look through all attributes and all attributes values in order to select the most optimal split-node. On the other hand, Random Forest will have to choose from a sample of features/attributes.\n",
        "- There is no prunning when building trees.\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1SDjlLuuELkcMI7cOTW3-Yg6Ke0BHPynw)\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1x7B1Bv8K9O-VDUpJAJSO1-k1gCiOHH8W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSjSElOPs-70"
      },
      "source": [
        "## Extremely Randomized Trees\n",
        "\n",
        "- Extremely Randomized Trees add another step of randomization in the way splits are computed\n",
        "- The same input training dataset is used to train all trees. (no bootstrap)\n",
        "- It essentially consists of randomizing strongly both attribute and attribute value's choice while splitting a tree node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9_5U2anlj89"
      },
      "source": [
        "#Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZwMhV_Zs0Jp"
      },
      "source": [
        "\n",
        "\n",
        "- Boosting consists in building a sequence of weak classifiers and adding them in the structure of the final strong classifier. \n",
        "- Weak learner (classifier) ‚Äì a classification algorithm with a substantial error rate which performance is not random\n",
        "- In other words, a weak leaner has an accuracy only slightly better than using random guessing\n",
        "- The weak classifiers are weighted based on the weak learners' accuracy.\n",
        "- Also data is reweighted after each weak classifier is built such as examples that are incorrectly classified gain some extra weight.\n",
        "- The result is that the next weak classifiers in the sequence focus more on the examples that previous weak classifiers missed\n",
        "\n",
        "![](http://drive.google.com/uc?export=view&id=1nm222fahzc3NOUe0ieaWLgD5VoJ-dvkI)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-5WSmRGtK4U"
      },
      "source": [
        "## Ada Boost\n",
        "\n",
        "- combination of **Weak Learners** that are slightly better than random guess\n",
        "- for binary classification, if the classification error is larger than $\\frac{1}{2}$, then we stop with finding Weak Learners\n",
        "- below you have the pseudocode for the original AdaBoost, but take into account that scikit-learn [implementation](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_weight_boosting.py) is based on a slightly modified version [SAMME](https://web.stanford.edu/~hastie/Papers/samme.pdf) for multiclass classification, that finally is reduced to AdaBoost if number of classes is 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJrI7Sa26WDe"
      },
      "source": [
        "![](http://drive.google.com/uc?export=view&id=1d8HPe8KkQIeGgPAJAbVdgqCPpG_RtcSG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYSbfw2dHIur"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loz-cys9M800"
      },
      "source": [
        "## Ex0. Download Titanic Dataset\n",
        "Columns:\n",
        "- 0: Survived Indicator\n",
        "- 1: Passenger Class\n",
        "- 2: Name\n",
        "- 3: Sex\n",
        "- 4: Age\n",
        "- 5: Siblings Aboard\n",
        "- 6: Parents Aboard\n",
        "- 7: Fare paid in ¬£s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7q6PytcjZuQ"
      },
      "source": [
        "!wget https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bEFUotSFtBt"
      },
      "source": [
        "## Ex1. Feature Engineering\n",
        "\n",
        "- Process data before applying classifiers\n",
        "  - check if there are null values in the dataset\n",
        "  - binning continous attributes for more efficiency\n",
        "  - remove column \"Name\"\n",
        "  - convert column \"Sex\" to 0/1\n",
        "  - split dataset into train/test\n",
        "  - for other preprocessing techniques you can access this [tutorial](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114) or this [book](https://github.com/yanshengjia/ml-road/blob/master/resources/Feature%20Engineering%20for%20Machine%20Learning.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyVHQlo9ltWQ"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def go(title, df_result):\n",
        "  df_str = df_result.to_string().split('\\n')\n",
        "  max_len = max(map(len, df_str))\n",
        "  half_len = int((max_len-len(title)-1)/2)\n",
        "  half_len = half_len if half_len else 1\n",
        "  print(\"-\" * half_len, title, \"-\" * half_len)\n",
        "  print(df_result.to_string())\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "large_titanic_df = pd.read_csv(\"titanic.csv\")\n",
        "go(\"Null Values\", large_titanic_df.isnull().sum())\n",
        "#large_titanic_df[\"Age\"] = pd.cut(large_titanic_df[\"Age\"], [0,10,20,30,40,50,60,70,80], \n",
        "#                           labels=[ 'Age=Child', 'Age=Adult', 'Age=Senior'])\n",
        "\n",
        "#titanic_df = large_titanic_df[['Sex', 'Age', 'Survived', 'Pclass']].copy()\n",
        "\n",
        "large_titanic_df['Age'] = large_titanic_df['Age'].apply(lambda x: int(x / 10) * 10)\n",
        "#titanic_df = titanic_df.drop('Name', axis=0)\n",
        "#data = pd.DataFrame(titanic_df)\n",
        "data = large_titanic_df.drop('Name', axis=1)\n",
        "data.head()\n",
        "data['Sex'] = data['Sex'].apply(lambda x: 0 if x == 'male' else 1)\n",
        "data.head()\n",
        "\n",
        "y = data.iloc[:,0]\n",
        "x = data.iloc[:,1:]\n",
        "print(x.head())\n",
        "print(y.head())\n",
        "#x = data.drop(['Fare', 'Parents/Children Aboard'], axis=1)\n",
        "#y = data[['Fare', 'Parents/Children Aboard']] \n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state=20)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDmm-EUqPclj"
      },
      "source": [
        "## Ex2. Decision Tree\n",
        "\n",
        "- use Decision Tree Classifier to check is passengers survived or not\n",
        "- check if **max_depth** parameter improves accuracy\n",
        "- compute confusion matrix and accuracy for train vs test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC5KHSurlt0x"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=3)\n",
        "clf = clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "res = confusion_matrix(y_test, y_pred)\n",
        "plot_confusion_matrix(res)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "y_pred=clf.predict(X_train)\n",
        "res = confusion_matrix(y_train, y_pred)\n",
        "plot_confusion_matrix(res)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_train, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHV2xk_PWsOx"
      },
      "source": [
        "## Ex3. Random Forest\n",
        "\n",
        "- use Random Forest to check is passengers survived or not\n",
        "- compute confusion matrix and accuracy for train vs test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbJ0BBPkluQK"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "clf=RandomForestClassifier()\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "y_pred=clf.predict(X_test)\n",
        "res = confusion_matrix(y_test, y_pred)\n",
        "plot_confusion_matrix(res)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "y_pred=clf.predict(X_train)\n",
        "res = confusion_matrix(y_train, y_pred)\n",
        "plot_confusion_matrix(res)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_train, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF5oD21AR7tn"
      },
      "source": [
        "## Ex4. AdaBoost\n",
        "\n",
        "- use AdaBoost to check is passengers survived or not\n",
        "- compute confusion matrix and accuracy for train vs test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOYDpA0rluqu"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "abc = AdaBoostClassifier(n_estimators=50,\n",
        "                         learning_rate=1)\n",
        "# Train Adaboost Classifer\n",
        "model = abc.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = model.predict(X_test)\n",
        "res = confusion_matrix(y_test, y_pred)\n",
        "plot_confusion_matrix(res)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "y_pred=clf.predict(X_train)\n",
        "res = confusion_matrix(y_train, y_pred)\n",
        "plot_confusion_matrix(res)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_train, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk0nmeunlblj"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "[Copyright stuff](https://docs.google.com/document/d/1v7Rddbjfhb2D29vsalKZoE-WSqTwbnTZm9a-FaLYWEY/edit?usp=sharing)"
      ]
    }
  ]
}